{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.image_object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import logging\n",
    "from os.path import join, isfile\n",
    "from functools import partial\n",
    "from mlcore.core import assign_arg_prefix, input_feedback\n",
    "from mlcore.annotation.core import AnnotationAdapter, SubsetType\n",
    "from mlcore.dataset.image_classification import ImageClassificationDataset\n",
    "from mlcore.image.pillow_tools import assign_exif_orientation\n",
    "from mlcore.annotation.core import RegionShape, convert_region\n",
    "from mlcore.tensorflow.tfrecord_builder import create_tfrecord_file\n",
    "from mlcore.evaluation.core import box_area, intersection_box, union_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for image object detection\n",
    "\n",
    "> Creates a dataset for image object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The data-set is created based on an image-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-sets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[image_set_type]*/*[image_set_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[image_set_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the image-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Set Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[data_set_type]*/*[data_set_name]*\n",
    "where `[data_set_type]` is the same as the corresponding `[image_set_type]` and `[data_set_name]` is the same as the corresponding `[image_set_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[data_set_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the data-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a object detection data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection data-set can be created from a segmentation or object-detection image-set.\n",
    "All images are validated against the annotations, if they contain at least one annotation and that the annotation category belongs to one of the given categories. The annotations have to be in [VIA v1](http://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.5.html) json format. Polygon annotations are converted into rectangle annotations for unique bounding-box generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ImageObjectDetectionDataset(ImageClassificationDataset):\n",
    "    \"\"\"\n",
    "    Object detection dataset.\n",
    "    `name`: The name of the dataset.\n",
    "    `base_path`: The data-set base-path.\n",
    "    `imageset_path`: The imageset source path.\n",
    "    `categories_path`: The path to the categories.txt file.\n",
    "    `annotations_path`: The path to the annotations-file.\n",
    "    `create_tfrecord`: Also create .tfrecord files.\n",
    "    `join_overlapping_regions`: Whether overlapping regions of same category should be joined.\n",
    "    `annotation_area_threshold`: Keep only annotations with minimum size (width or height) related to image size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_adapter: AnnotationAdapter, output_adapter: AnnotationAdapter, split=None, seed=None,\n",
    "                 sample=None, tfrecord=False, join_overlapping_regions=False, annotation_area_threshold=None):\n",
    "        super().__init__(input_adapter, output_adapter, split, seed, sample, tfrecord)\n",
    "        self.join_overlapping_regions = join_overlapping_regions\n",
    "        self.annotation_area_threshold = annotation_area_threshold\n",
    "\n",
    "    @classmethod\n",
    "    def argparse(cls, prefix=None):\n",
    "        \"\"\"\n",
    "        Returns the argument parser containing argument definition for command line use.\n",
    "        `prefix`: a parameter prefix to set, if needed\n",
    "        return: the argument parser\n",
    "        \"\"\"\n",
    "        parser = super(ImageObjectDetectionDataset, cls).argparse(prefix=prefix)\n",
    "        parser.add_argument(assign_arg_prefix(\"--join_overlapping_regions\", prefix),\n",
    "                            dest=\"join_overlapping_regions\",\n",
    "                            help=\"Whether overlapping regions of same category should be joined.\",\n",
    "                            action=\"store_true\")\n",
    "        parser.add_argument(assign_arg_prefix(\"--annotation_area_threshold\", prefix),\n",
    "                            dest=\"annotation_area_threshold\",\n",
    "                            help=\"Keep only annotations with minimum size (width or height) related to image size.\",\n",
    "                            type=float,\n",
    "                            default=None)\n",
    "        return parser\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates the annotations.\n",
    "        return: The skipped annotations\n",
    "        \"\"\"\n",
    "        # convert the annotations before doing validation\n",
    "        self.convert_annotations()\n",
    "        return super().validate()\n",
    "\n",
    "    def copy(self, train_annotation_keys, val_annotation_keys, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the dataset and remove EXIF orientation information by hard-rotate the images.\n",
    "        If tfrecords should be build, create tfrecords for train and val subsets and generate a labelmap.pbtxt file.\n",
    "        `train_annotation_keys`: The list of training annotation keys\n",
    "        `val_annotation_keys`: The list of validation annotation keys\n",
    "        `test_files`: The list of test file paths\n",
    "        return: A tuple containing train, val and test target file paths\n",
    "        \"\"\"\n",
    "\n",
    "        train_targets, val_targets, test_targets = super().copy(train_annotation_keys, val_annotation_keys, test_files)\n",
    "\n",
    "        # if creating a .tfrecord\n",
    "        if self.tfrecord:\n",
    "            # write train subset tfrecords\n",
    "            if train_annotation_keys:\n",
    "                annotations = dict(zip(train_annotation_keys, [self.annotations[key] for key in train_annotation_keys]))\n",
    "                tfrecord_output_file = join(self.output_adapter.path, '{}.record'.format(str(SubsetType.TRAIN)))\n",
    "                logger.info('Generate file {}'.format(tfrecord_output_file))\n",
    "                create_tfrecord_file(tfrecord_output_file, self.categories, annotations)\n",
    "\n",
    "            # write val subset tfrecords\n",
    "            if val_annotation_keys:\n",
    "                annotations = dict(zip(val_annotation_keys, [self.annotations[key] for key in val_annotation_keys]))\n",
    "                tfrecord_output_file = join(self.output_adapter.path, '{}.record'.format(str(SubsetType.VAL)))\n",
    "                logger.info('Generate file {}'.format(tfrecord_output_file))\n",
    "                create_tfrecord_file(tfrecord_output_file, self.categories, annotations)\n",
    "\n",
    "        return train_targets, val_targets, test_targets\n",
    "\n",
    "    def convert_annotations(self):\n",
    "        \"\"\"\n",
    "        Converts segmentation regions from polygon to rectangle, if exist\n",
    "        \"\"\"\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        area_threshold = self.annotation_area_threshold\n",
    "\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'position',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    't': 'Trim',  # transform the annotation\n",
    "                    'T': 'Trim All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: p_min < 0 or p_max >= size,\n",
    "                'message': '{} -> {} : {}Exceeds image {}. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: max(min(p, size - 1), 0),\n",
    "            },\n",
    "            {\n",
    "                'name': 'size',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, _: p_max - p_min <= 1,\n",
    "                'message': '{} -> {} : {}Shape {} is <= 1 pixel. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: p,\n",
    "            },\n",
    "            {\n",
    "                'name': 'area',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: area_threshold and (p_max - p_min) / size <= area_threshold,\n",
    "                'message': '{} <= {} percent. {}'.format('{} -> {} : {}Shape {} is', (area_threshold or 0) * 100,\n",
    "                                                         ' \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}'),\n",
    "                'transform': lambda p, size=0: p,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        logger.info('Start convert image annotations from {}'.format(self.input_adapter.path))\n",
    "\n",
    "        for annotation in self.annotations.values():\n",
    "            # skip file, if regions are empty or file do not exist\n",
    "            if not (annotation.regions and isfile(annotation.file_path)):\n",
    "                continue\n",
    "\n",
    "            # convert from polygon to rect if needed\n",
    "            for region in annotation.regions:\n",
    "                convert_region(region, RegionShape.RECTANGLE)\n",
    "\n",
    "            # try to join regions\n",
    "            if self.join_overlapping_regions:\n",
    "                self._join_regions(annotation.regions)\n",
    "\n",
    "            image, _, __ = assign_exif_orientation(annotation.file_path)\n",
    "            img_width, img_height = image.size\n",
    "            delete_regions = {}\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                # validate the shape size\n",
    "                x_min, x_max = region.points_x[:2]\n",
    "                y_min, y_max = region.points_y[:2]\n",
    "                for step in steps:\n",
    "                    width_condition = step['condition'](x_min, x_max, img_width)\n",
    "                    height_condition = step['condition'](y_min, y_max, img_height)\n",
    "                    if width_condition or height_condition:\n",
    "                        size_message = ['width'] if width_condition else []\n",
    "                        size_message.extend(['height'] if height_condition else [])\n",
    "                        message = step['message'].format(annotation.file_path, index, ' ',\n",
    "                                                         ' and '.join(size_message),\n",
    "                                                         x_min, y_min, x_max, y_max)\n",
    "\n",
    "                        step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                        choice_op = step['choice'].lower()\n",
    "                        # if skip the shapes\n",
    "                        if choice_op == 's':\n",
    "                            delete_regions[index] = True\n",
    "                            message = step['message'].format(annotation.file_path, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             x_min, y_min, x_max, y_max)\n",
    "                            logger.info(message)\n",
    "\n",
    "                            break\n",
    "                        else:\n",
    "                            region.points_x = list(map(partial(step['transform'], size=img_width), [x_min, x_max]))\n",
    "                            region.points_y = list(map(partial(step['transform'], size=img_height), [y_min, y_max]))\n",
    "                            message = step['message'].format(annotation.file_path, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             x_min, y_min, x_max, y_max)\n",
    "                            logger.info(message)\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for index in sorted(list(delete_regions.keys()), reverse=True):\n",
    "                del annotation.regions[index]\n",
    "\n",
    "        logger.info('Finished convert image annotations from {}'.format(self.input_adapter.path))\n",
    "\n",
    "    @classmethod\n",
    "    def _join_regions(cls, regions):\n",
    "        \"\"\"\n",
    "        Join regions which overlaps.\n",
    "        `regions`: the region to parse\n",
    "        \"\"\"\n",
    "        len_before = len(regions)\n",
    "        index_left = 0\n",
    "        while index_left < len(regions):\n",
    "            regions_joined = []\n",
    "            region_left = regions[index_left]\n",
    "            for index_right in range(len(regions)):\n",
    "                if index_left == index_right:\n",
    "                    continue\n",
    "                region_right = regions[index_right]\n",
    "                same_label_length = len(region_left.labels) == len(region_right.labels)\n",
    "                same_label_contents = len(region_left.labels) == len(set(region_left.labels) & set(region_right.labels))\n",
    "                same_labels = same_label_length and same_label_contents\n",
    "                if same_labels:\n",
    "                    bbox_left = (region_left.points_x, region_left.points_y)\n",
    "                    bbox_right = (region_right.points_x, region_right.points_y)\n",
    "                    inter_area = box_area(intersection_box(bbox_left, bbox_right))\n",
    "                    if inter_area > 0:\n",
    "                        points_x, points_y = union_box(bbox_left, bbox_right)\n",
    "                        region_left.points_x = points_x\n",
    "                        region_left.points_y = points_y\n",
    "                        regions_joined.append(index_right)\n",
    "            for index in regions_joined[::-1]:\n",
    "                del regions[index]\n",
    "            if not regions_joined:\n",
    "                index_left += 1\n",
    "        logger.info('Joined overlapping regions from {} -> {}.'.format(len_before, len(regions)))\n",
    "\n",
    "    def build_info(self):\n",
    "        \"\"\"\n",
    "        Converts annotations\n",
    "        \"\"\"\n",
    "        super().build_info()\n",
    "        logger.info('join_overlapping_regions: {}'.format(self.join_overlapping_regions))\n",
    "        logger.info('annotation_area_threshold: {}'.format(self.annotation_area_threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
