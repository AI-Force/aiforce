{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.image_object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from os.path import join, isfile, dirname, normpath, basename\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from mlcore.dataset.core import Dataset, input_feedback, configure_logging\n",
    "from mlcore.dataset.type import DatasetType\n",
    "from mlcore.image.pillow_tools import assign_exif_orientation\n",
    "from mlcore.annotation.core import RegionShape, convert_region\n",
    "from mlcore.annotation.via_adapter import read_annotations as via_read_annotations\n",
    "from mlcore.annotation.via_adapter import write_annotations as via_write_annotations\n",
    "from mlcore.tensorflow.tfrecord_builder import create_tfrecord_file\n",
    "from mlcore.evaluation.core import box_area, intersection_box, union_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "CATEGORY_LABEL_KEY = 'category'\n",
    "DEFAULT_CATEGORIES_FILE = 'categories.txt'\n",
    "DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE = 'annotations.csv'\n",
    "DEFAULT_SEGMENTATION_ANNOTATIONS_FILE = 'via_region_data.json'\n",
    "DEFAULT_SPLIT = 0.2\n",
    "DATA_SET_FOLDER = 'datasets'\n",
    "SEMANTIC_MASK_FOLDER = 'semantic_masks'\n",
    "TRAIN_VAL_FOLDER = 'trainval'\n",
    "TRAIN_FOLDER = 'train'\n",
    "VAL_FOLDER = 'val'\n",
    "TEST_FOLDER = 'test'\n",
    "NOT_CATEGORIZED = '[NOT_CATEGORIZED]'\n",
    "\n",
    "DATASET_TYPE = DatasetType.IMAGE_OBJECT_DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for image object detection\n",
    "\n",
    "> Creates a dataset for image object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The data-set is created based on an image-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-sets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[image_set_type]*/*[image_set_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[image_set_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the image-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Set Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[data_set_type]*/*[data_set_name]*\n",
    "where `[data_set_type]` is the same as the corresponding `[image_set_type]` and `[data_set_name]` is the same as the corresponding `[image_set_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[data_set_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the data-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a object detection data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection data-set can be created from a segmentation or object-detection image-set.\n",
    "All images are validated against the annotations, if they contain at least one annotation and that the annotation category belongs to one of the given categories. The annotations have to be in [VIA v1](http://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.5.html) json format. Polygon annotations are converted into rectangle annotations for unique bounding-box generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ImageObjectDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Object detection dataset.\n",
    "    `name`: The name of the dataset.\n",
    "    `base_path`: The data-set base-path.\n",
    "    `imageset_path`: The imageset source path.\n",
    "    `categories_path`: The path to the categories.txt file.\n",
    "    `annotations_path`: The path to the annotations-file.\n",
    "    `create_tfrecord`: Also create .tfrecord files.\n",
    "    `join_overlapping_regions`: Whether overlapping regions of same category should be joined.\n",
    "    `annotation_area_threshold`: Keep only annotations with minimum size (width or height) related to image size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, base_path, imageset_path, categories_path, annotations_path=None,\n",
    "                 create_tfrecord=False, join_overlapping_regions=False, annotation_area_threshold=None):\n",
    "        super().__init__(name, base_path, imageset_path, categories_path, DATASET_TYPE, create_tfrecord)\n",
    "        self.annotations_path = annotations_path\n",
    "        self.annotations = via_read_annotations(annotations_path, self.train_val_folder, CATEGORY_LABEL_KEY) if annotations_path else {}\n",
    "        self.join_overlapping_regions = join_overlapping_regions\n",
    "        self.annotation_area_threshold = annotation_area_threshold\n",
    "\n",
    "    def copy(self, train_file_keys, val_file_keys, test_file_names=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set, generate the annotations for train and val images.\n",
    "        `train_file_keys`: The list of training image keys\n",
    "        `val_file_keys`: The list of validation image keys\n",
    "        `test_file_names`: The list of test image file names\n",
    "        return: A tuple containing train and val annotations\n",
    "        \"\"\"\n",
    "\n",
    "        annotations_train, annotations_val = super().copy(train_file_keys, val_file_keys, test_file_names)\n",
    "\n",
    "        # write the split train annotations\n",
    "        if annotations_train:\n",
    "            annotations_target_path = join(self.train_folder, DEFAULT_SEGMENTATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            via_write_annotations(annotations_target_path, annotations_train, CATEGORY_LABEL_KEY)\n",
    "            # if creating a .tfrecord\n",
    "            if self.create_tfrecord:\n",
    "                tfrecord_file_name = '{}.record'.format(normpath(basename(self.train_folder)))\n",
    "                tfrecord_output_file = join(self.folder, tfrecord_file_name)\n",
    "                self.logger.info('Generate file {} to {}'.format(tfrecord_file_name, self.folder))\n",
    "                create_tfrecord_file(tfrecord_output_file, self.categories, annotations_train)\n",
    "\n",
    "        # write the split val annotations\n",
    "        if annotations_val:\n",
    "            annotations_target_path = join(self.val_folder, DEFAULT_SEGMENTATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            via_write_annotations(annotations_target_path, annotations_val, CATEGORY_LABEL_KEY)\n",
    "            # if creating a .tfrecord\n",
    "            if self.create_tfrecord:\n",
    "                tfrecord_file_name = '{}.record'.format(normpath(basename(self.val_folder)))\n",
    "                tfrecord_output_file = join(self.folder, tfrecord_file_name)\n",
    "                self.logger.info('Generate file {} to {}'.format(tfrecord_file_name, self.folder))\n",
    "                create_tfrecord_file(tfrecord_output_file, self.categories, annotations_val)\n",
    "\n",
    "        return annotations_train, annotations_val\n",
    "\n",
    "    def convert_annotations(self):\n",
    "        \"\"\"\n",
    "        Converts segmentation regions from polygon to rectangle, if exist\n",
    "        \"\"\"\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        area_threshold = self.annotation_area_threshold\n",
    "\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'position',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    't': 'Trim',  # transform the annotation\n",
    "                    'T': 'Trim All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: p_min < 0 or p_max >= size,\n",
    "                'message': '{} -> {} : {}Exceeds image {}. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: max(min(p, size - 1), 0),\n",
    "            },\n",
    "            {\n",
    "                'name': 'size',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, _: p_max - p_min <= 1,\n",
    "                'message': '{} -> {} : {}Shape {} is <= 1 pixel. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: p,\n",
    "            },\n",
    "            {\n",
    "                'name': 'area',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: area_threshold and (p_max - p_min) / size <= area_threshold,\n",
    "                'message': '{} <= {} percent. {}'.format('{} -> {} : {}Shape {} is', (area_threshold or 0) * 100, ' \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}'),\n",
    "                'transform': lambda p, size=0: p,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.logger.info('Start convert image annotations from {}'.format(self.annotations_path))\n",
    "\n",
    "        for annotation in self.annotations.values():\n",
    "            # skip file, if regions are empty or file do not exist\n",
    "            if not (annotation.regions and isfile(annotation.file_path)):\n",
    "                continue\n",
    "\n",
    "            # convert from polygon to rect if needed\n",
    "            for region in annotation.regions:\n",
    "                convert_region(region, RegionShape.RECTANGLE)\n",
    "\n",
    "            # try to join regions\n",
    "            if self.join_overlapping_regions:\n",
    "                self.join_regions(annotation.regions)\n",
    "\n",
    "            image, _, __ = assign_exif_orientation(annotation.file_path)\n",
    "            img_width, img_height = image.size\n",
    "            delete_regions = {}\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                # validate the shape size\n",
    "                x_min, x_max = region.points_x[:2]\n",
    "                y_min, y_max = region.points_y[:2]\n",
    "                for step in steps:\n",
    "                    width_condition = step['condition'](x_min, x_max, img_width)\n",
    "                    height_condition = step['condition'](y_min, y_max, img_height)\n",
    "                    if width_condition or height_condition:\n",
    "                        size_message = ['width'] if width_condition else []\n",
    "                        size_message.extend(['height'] if height_condition else [])\n",
    "                        message = step['message'].format(annotation.file_name, index, ' ',\n",
    "                                                         ' and '.join(size_message),\n",
    "                                                         x_min, y_min, x_max, y_max)\n",
    "\n",
    "                        step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                        choice_op = step['choice'].lower()\n",
    "                        # if skip the shapes\n",
    "                        if choice_op == 's':\n",
    "                            delete_regions[index] = True\n",
    "                            message = step['message'].format(annotation.file_name, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             x_min, y_min, x_max, y_max)\n",
    "                            self.logger.info(message)\n",
    "\n",
    "                            break\n",
    "                        else:\n",
    "                            region.points_x = list(map(partial(step['transform'], size=img_width), [x_min, x_max]))\n",
    "                            region.points_y = list(map(partial(step['transform'], size=img_height), [y_min, y_max]))\n",
    "                            message = step['message'].format(annotation.file_name, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             x_min, y_min, x_max, y_max)\n",
    "                            self.logger.info(message)\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for index in sorted(list(delete_regions.keys()), reverse=True):\n",
    "                del annotation.regions[index]\n",
    "\n",
    "        self.logger.info('Finished convert image annotations from {}'.format(self.annotations_path))\n",
    "\n",
    "    def join_regions(self, regions):\n",
    "        \"\"\"\n",
    "        Join regions which overlaps.\n",
    "        `regions`: the region to parse\n",
    "        \"\"\"\n",
    "        len_before = len(regions)\n",
    "        index_left = 0\n",
    "        while index_left < len(regions):\n",
    "            regions_joined = []\n",
    "            region_left = regions[index_left]\n",
    "            for index_right in range(len(regions)):\n",
    "                if index_left == index_right:\n",
    "                    continue\n",
    "                region_right = regions[index_right]\n",
    "                same_label_length = len(region_left.labels) == len(region_right.labels)\n",
    "                same_labels = same_label_length and len(region_left.labels) == len(set(region_left.labels) & set(region_right.labels))\n",
    "                if same_labels:\n",
    "                    bbox_left = (region_left.points_x, region_left.points_y)\n",
    "                    bbox_right = (region_right.points_x, region_right.points_y)\n",
    "                    inter_area = box_area(intersection_box(bbox_left, bbox_right))\n",
    "                    if inter_area > 0:\n",
    "                        points_x, points_y = union_box(bbox_left, bbox_right)\n",
    "                        region_left.points_x = points_x\n",
    "                        region_left.points_y = points_y\n",
    "                        regions_joined.append(index_right)\n",
    "            for index in regions_joined[::-1]:\n",
    "                del regions[index]\n",
    "            if not regions_joined:\n",
    "                index_left += 1\n",
    "        self.logger.info('Joined overlapping regions from {} -> {}.'.format(len_before, len(regions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data-set from an image-set. Handles currently classification and segmentation image-sets taken from the image-set-type, which is the parent folder, the image-set folder is located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def build_dataset(category_file_path, output, annotation_file_path=None, split=DEFAULT_SPLIT, seed=None, sample=0,\n",
    "                  create_tfrecord=False, join_overlapping_regions=False, annotation_area_threshold=None,\n",
    "                  dataset_name=None):\n",
    "    \"\"\"\n",
    "    Build the dataset for training, Validation and test\n",
    "    `category_file_path`: the filename of the categories file\n",
    "    `output`: the dataset base folder to build the dataset in\n",
    "    `annotation_file_path`: the file path to the annotation file\n",
    "    `split`: the size of the validation set as percentage\n",
    "    `seed`: random seed to reproduce splits\n",
    "    `sample`: the size of the sample set as percentage\n",
    "    `create_tfrecord`: Also create .tfrecord files.\n",
    "    `join_overlapping_regions`: Whether overlapping regions of same category should be joined.\n",
    "    `annotation_area_threshold`: Keep only annotations with minimum size (width or height) related to image size\n",
    "    `dataset_name`: the name of the dataset, if not set infer from the category file path\n",
    "    \"\"\"\n",
    "    log_memory_handler = configure_logging()\n",
    "\n",
    "    path = dirname(category_file_path)\n",
    "\n",
    "    # try to infer the data-set name if not explicitly set\n",
    "    if dataset_name is None:\n",
    "        dataset_name = basename(path)\n",
    "\n",
    "    logger.info('Build parameters:')\n",
    "    logger.info(' '.join(sys.argv[1:]))\n",
    "    logger.info('Build configuration:')\n",
    "    logger.info('category_file_path: {}'.format(category_file_path))\n",
    "    logger.info('annotation_file_path: {}'.format(annotation_file_path))\n",
    "    logger.info('split: {}'.format(split))\n",
    "    logger.info('seed: {}'.format(seed))\n",
    "    logger.info('sample: {}'.format(sample))\n",
    "    logger.info('dataset_type: {}'.format(DATASET_TYPE))\n",
    "    logger.info('output: {}'.format(output))\n",
    "    logger.info('join_overlapping_regions: {}'.format(join_overlapping_regions))\n",
    "    logger.info('annotation_area_threshold: {}'.format(annotation_area_threshold))\n",
    "    logger.info('name: {}'.format(dataset_name))\n",
    "\n",
    "    logger.info('Start build {} data-set {} at {}'.format(DATASET_TYPE, dataset_name, output))\n",
    "\n",
    "    dataset = ImageObjectDetectionDataset(dataset_name, output, path, category_file_path, annotation_file_path,\n",
    "                                          create_tfrecord, join_overlapping_regions, annotation_area_threshold)\n",
    "\n",
    "    # create the dataset folders\n",
    "    logger.info(\"Start create the dataset folders at {}\".format(dataset.base_path))\n",
    "    dataset.create_folders()\n",
    "    logger.info(\"Finished create the dataset folders at {}\".format(dataset.base_path))\n",
    "\n",
    "    # create the build log file\n",
    "    log_file_name = datetime.now().strftime(\"build_%Y.%m.%d-%H.%M.%S.log\")\n",
    "    file_handler = logging.FileHandler(join(dataset.folder, log_file_name), encoding=\"utf-8\")\n",
    "    log_memory_handler.setTarget(file_handler)\n",
    "\n",
    "    # build the dataset\n",
    "    dataset.build(split, seed, sample)\n",
    "\n",
    "    logger.info('Finished build {} dataset {} at {}'.format(DATASET_TYPE, dataset_name, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the dataset builder from command line, use the following command:\n",
    "`python -m mlcore.dataset.image_object_detection [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `[categories]`: The path to the categories file. (e.g.: *categories.txt*)\n",
    "- `--annotation`: The path to the image-set annotation file, the data-set is build from. (e.g.: *imagesets/classification/car_damage/annotations.csv* for classification, *imagesets/segmentation/car_damage/via_region_data.json* for segmentation)\n",
    "- `--split`: The percentage of the data which belongs to validation set, default to *0.2* (=20%)\n",
    "- `--seed`: A random seed to reproduce splits, default to None\n",
    "- `--category-label-key`: The key, the category name can be found in the annotation file, default to *category*.\n",
    "- `--sample`: The percentage of the data which will be copied as a sample set with in a separate folder with \"_sample\" suffix. If not set, no sample data-set will be created.\n",
    "- `--tfrecord`: Also create .tfrecord files.\n",
    "- `--join-overlapping-regions`: Whether overlapping regions of same category should be joined.\n",
    "- `--annotation-area-thresh`: Keep only annotations with minimum size (width or height) related to image size.\n",
    "- `--output`: The path of the dataset folder, default to *../datasets*.\n",
    "- `--name`: The name of the data-set, if not explicitly set try to infer from categories file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"categories\",\n",
    "                        help=\"The path to the image-set categories file.\")\n",
    "    parser.add_argument(\"--annotation\",\n",
    "                        help=\"The path to the image-set annotation file, the data-set is build from.\",\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--split\",\n",
    "                        help=\"Percentage of the data which belongs to validation set.\",\n",
    "                        type=float,\n",
    "                        default=0.2)\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        help=\"A random seed to reproduce splits.\",\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--category-label-key\",\n",
    "                        help=\"The key of the category name.\",\n",
    "                        default=CATEGORY_LABEL_KEY)\n",
    "    parser.add_argument(\"--sample\",\n",
    "                        help=\"Percentage of the data which will be copied as a sample set.\",\n",
    "                        type=float,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--tfrecord\",\n",
    "                        help=\"Also create .tfrecord files.\",\n",
    "                        action=\"store_true\")\n",
    "    parser.add_argument(\"--join-overlapping-regions\",\n",
    "                        help=\"Whether overlapping regions of same category should be joined.\",\n",
    "                        action=\"store_true\")\n",
    "    parser.add_argument(\"--annotation-area-thresh\",\n",
    "                        help=\"Keep only annotations with minimum size (width or height) related to image size.\",\n",
    "                        type=float,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--output\",\n",
    "                        help=\"The path of the data-set folder.\",\n",
    "                        default=DATA_SET_FOLDER)\n",
    "    parser.add_argument(\"--name\",\n",
    "                        help=\"The name of the data-set, if not explicitly set try to infer from categories file path.\",\n",
    "                        default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    CATEGORY_LABEL_KEY = args.category_label_key\n",
    "\n",
    "    build_dataset(args.categories, args.output, args.annotation, args.split, args.seed, args.sample, args.tfrecord,\n",
    "                   args.join_overlapping_regions, args.annotation_area_thresh, args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
