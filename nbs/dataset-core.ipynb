{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "import logging\n",
    "from logging.handlers import MemoryHandler\n",
    "from os.path import isdir, join, basename\n",
    "from mlcore.image.pillow_tools import assign_exif_orientation, write_exif_metadata\n",
    "from mlcore.io.core import create_folder, scan_files\n",
    "from mlcore.tensorflow.tfrecord_builder import create_labelmap_file\n",
    "from mlcore.category_tools import read_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "DEFAULT_CATEGORIES_FILE = 'categories.txt'\n",
    "DEFAULT_SPLIT = 0.2\n",
    "TRAIN_VAL_FOLDER = 'trainval'\n",
    "TRAIN_FOLDER = 'train'\n",
    "VAL_FOLDER = 'val'\n",
    "TEST_FOLDER = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dataset Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The dataset is created based on an imageset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imageset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagesets are collected images to build a dataset from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[imageset_type]*/*[imageset_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[imageset_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the imageset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[dataset_type]*/*[dataset_name]*\n",
    "where `[dataset_type]` is the same as the corresponding `[imageset_type]` and `[dataset_name]` is the same as the corresponding `[imageset_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[dataset_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the dataset contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, name, base_path, imageset_path, categories_path, dataset_type, create_tfrecord=False):\n",
    "        self.name = name\n",
    "        self.base_path = base_path\n",
    "        self.imageset_path = imageset_path\n",
    "        self.train_val_folder = join(imageset_path, TRAIN_VAL_FOLDER)\n",
    "        self.test_source_folder = join(imageset_path, TEST_FOLDER)\n",
    "        self.categories_path = categories_path\n",
    "        self.categories = read_categories(categories_path, dataset_type)\n",
    "        self.dataset_type = dataset_type\n",
    "        self.create_tfrecord = create_tfrecord\n",
    "        self.folder = None\n",
    "        self.train_folder = None\n",
    "        self.val_folder = None\n",
    "        self.test_target_folder = None\n",
    "        self.logger = self.get_logger()\n",
    "        self.annotations = None\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"\n",
    "        Creates the dataset folder structure, if not exist\n",
    "        \"\"\"\n",
    "\n",
    "        # create data-set folder and remove previous data if exist\n",
    "        self.folder = create_folder(join(self.base_path, str(self.dataset_type), self.name), clear=True)\n",
    "        self.logger.info(\"Created folder {}\".format(self.folder))\n",
    "        self.train_folder = create_folder(join(self.folder, TRAIN_FOLDER))\n",
    "        self.logger.info(\"Created folder {}\".format(self.train_folder))\n",
    "        self.val_folder = create_folder(join(self.folder, VAL_FOLDER))\n",
    "        self.logger.info(\"Created folder {}\".format(self.val_folder))\n",
    "        if isdir(self.test_source_folder):\n",
    "            self.test_target_folder = create_folder(join(self.folder, TEST_FOLDER))\n",
    "            self.logger.info(\"Created folder {}\".format(self.test_target_folder))\n",
    "\n",
    "    def convert_annotations(self):\n",
    "        \"\"\"\n",
    "        Converts annotations\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates, that each file has at least one annotation.\n",
    "        \"\"\"\n",
    "\n",
    "        # validate only the trainval images, the test images have no annotations to validate\n",
    "        # convert the annotations before doing validation\n",
    "        self.convert_annotations()\n",
    "\n",
    "        self.logger.info('Start validate imageset at {}'.format(self.train_val_folder))\n",
    "\n",
    "        files = scan_files(self.train_val_folder)\n",
    "\n",
    "        self.logger.info('Found {} files at {}'.format(len(files), self.train_val_folder))\n",
    "\n",
    "        delete_annotations = {}\n",
    "        used_categories = set([])\n",
    "\n",
    "        for annotation_id, annotation in self.annotations.items():\n",
    "            delete_regions = {}\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                len_labels = len(region.labels)\n",
    "                region_valid = len_labels and len(set(region.labels) & set(self.categories)) == len_labels\n",
    "                if not region_valid:\n",
    "                    message = '{} : Region {} with category {} is not in category list, skip region.'\n",
    "                    self.logger.info(message.format(annotation.file_name, index, ','.join(region.labels)))\n",
    "                    delete_regions[index] = True\n",
    "                else:\n",
    "                    # update the used regions\n",
    "                    used_categories.update(region.labels)\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for index in sorted(list(delete_regions.keys()), reverse=True):\n",
    "                del annotation.regions[index]\n",
    "\n",
    "            # validate for empty region\n",
    "            if not annotation.regions:\n",
    "                self.logger.info('{} : Has empty regions, skip annotation.'.format(annotation.file_path))\n",
    "                delete_annotations[annotation_id] = True\n",
    "            # validate for file exist\n",
    "            elif annotation.file_path not in files:\n",
    "                self.logger.info('{} : File of annotations do not exist, skip annotations.'.format(annotation.file_path))\n",
    "                delete_annotations[annotation_id] = True\n",
    "            else:\n",
    "                files.pop(files.index(annotation.file_path))\n",
    "\n",
    "        for index, file in enumerate(files):\n",
    "            self.logger.info('[{}] -> {} : File has no annotations, skip file.'.format(index, file))\n",
    "\n",
    "        # list unised categories\n",
    "        empty_categories = frozenset(self.categories) - used_categories\n",
    "        if empty_categories:\n",
    "            self.logger.info('The following categories have no images: {}'.format(\" , \".join(empty_categories)))\n",
    "\n",
    "        # delete annotations after iteration is finished\n",
    "        for index in delete_annotations.keys():\n",
    "            del self.annotations[index]\n",
    "\n",
    "        self.logger.info('Finished validate image set at {}'.format(self.train_val_folder))\n",
    "\n",
    "    def copy(self, train_file_keys, val_file_keys, test_file_names=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the dataset.\n",
    "        `train_file_keys`: The list of training image keys\n",
    "        `val_file_keys`: The list of validation image keys\n",
    "        `test_file_names`: The list of test image file names\n",
    "        return: A tuple containing train and val annotations\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info('Start copy files from {} to {}'.format(self.imageset_path, self.folder))\n",
    "\n",
    "        # copy the categories files\n",
    "        self.logger.info('Copy file {} to {}'.format(self.categories_path, self.folder))\n",
    "        shutil.copy2(self.categories_path, join(self.folder, DEFAULT_CATEGORIES_FILE))\n",
    "\n",
    "        # if create tfrecord, create a labelmap.pbtxt file containing the categories\n",
    "        if self.create_tfrecord:\n",
    "            labelmap_file_name = 'label_map.pbtxt'\n",
    "            labelmap_output_file = join(self.folder, labelmap_file_name)\n",
    "            self.logger.info('Generate file {} to {}'.format(labelmap_file_name, self.folder))\n",
    "            create_labelmap_file(labelmap_output_file, list(self.categories)[1:], 1)\n",
    "\n",
    "        annotations_train = {}\n",
    "        annotations_val = {}\n",
    "\n",
    "        num_files = len(train_file_keys)\n",
    "        self.logger.info('Start copy {} files to {}'.format(num_files, self.train_folder))\n",
    "\n",
    "        for key in train_file_keys:\n",
    "            # copy image\n",
    "            annotation = self.annotations[key]\n",
    "            copy_image_and_assign_orientation(self.train_val_folder, annotation.file_name, self.train_folder)\n",
    "\n",
    "            # add annotation\n",
    "            annotations_train[key] = annotation\n",
    "        self.logger.info('Finished copy {} files to {}'.format(num_files, self.train_folder))\n",
    "\n",
    "        num_files = len(val_file_keys)\n",
    "        self.logger.info('Start copy {} files to {}'.format(num_files, self.val_folder))\n",
    "\n",
    "        for key in val_file_keys:\n",
    "            # copy image\n",
    "            annotation = self.annotations[key]\n",
    "            copy_image_and_assign_orientation(self.train_val_folder, annotation.file_name, self.val_folder)\n",
    "\n",
    "            # add annotation\n",
    "            annotations_val[key] = annotation\n",
    "\n",
    "        self.logger.info('Finished copy {} files to {}'.format(num_files, self.val_folder))\n",
    "\n",
    "        # copy test_files, if exist\n",
    "        if test_file_names:\n",
    "\n",
    "            num_files = len(test_file_names)\n",
    "            self.logger.info('Start copy {} files to {}'.format(num_files, self.test_target_folder))\n",
    "\n",
    "            for file_name in test_file_names:\n",
    "                copy_image_and_assign_orientation(self.test_source_folder, file_name, self.test_target_folder)\n",
    "\n",
    "            self.logger.info('Finished copy {} files to {}'.format(num_files, self.test_target_folder))\n",
    "\n",
    "        self.logger.info('Finished copy files from {} to {}'.format(self.imageset_path, self.folder))\n",
    "\n",
    "        return annotations_train, annotations_val\n",
    "\n",
    "    def build(self, split=DEFAULT_SPLIT, seed=None, sample=None):\n",
    "        \"\"\"\n",
    "        Build the data-set. This is the main logic.\n",
    "        This method validates the images against the annotations,\n",
    "        split the image-set into train and val on given split percentage,\n",
    "        creates the data-set folders and copies the image.\n",
    "        If a sample percentage is given, a sub-set is created as sample.\n",
    "        `split`: The percentage of images which will be copied into the validation set\n",
    "        `seed`: A random seed to reproduce splits\n",
    "        `sample`: The percentage of images from train, val and test which will also from a sample set\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info('Validation set contains {}% of the images.'.format(int(split * 100)))\n",
    "\n",
    "        # validate the image set\n",
    "        self.validate()\n",
    "\n",
    "        # sort filenames by assigned tags\n",
    "        labels_to_keys = {}\n",
    "\n",
    "        for annotation_id, annotation in self.annotations.items():\n",
    "            labels = ' '.join(annotation.labels())\n",
    "\n",
    "            if labels not in labels_to_keys:\n",
    "                labels_to_keys[labels] = []\n",
    "            labels_to_keys[labels].append(annotation_id)\n",
    "\n",
    "        # split category files into train & val and create the sample split, if set\n",
    "        train_file_keys = []\n",
    "        val_file_keys = []\n",
    "        sample_train_file_keys = []\n",
    "        sample_val_file_keys = []\n",
    "\n",
    "        for annotation_ids in labels_to_keys.values():\n",
    "            train, val = split_train_val_data(annotation_ids, split, seed)\n",
    "            train_file_keys.extend(train)\n",
    "            val_file_keys.extend(val)\n",
    "\n",
    "            # if a sample data set should be created, create the splits\n",
    "            if sample:\n",
    "                _, sample_train = split_train_val_data(train, sample, seed)\n",
    "                _, sample_val = split_train_val_data(val, sample, seed)\n",
    "                sample_train_file_keys.extend(sample_train)\n",
    "                sample_val_file_keys.extend(sample_val)\n",
    "\n",
    "        # scan the test images if exist\n",
    "        test_file_names = list(map(lambda f: basename(f), scan_files(self.test_source_folder))) if self.test_target_folder else None\n",
    "        _, sample_test_file_names = split_train_val_data(test_file_names, sample, seed) if test_file_names and sample else (None, None)\n",
    "\n",
    "        # copy the files\n",
    "        self.copy(train_file_keys, val_file_keys, test_file_names)\n",
    "\n",
    "        if sample:\n",
    "            sample_name = \"{}_sample\".format(self.name)\n",
    "\n",
    "            self.logger.info('Start build {} data-set containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                             int(sample * 100),\n",
    "                                                                                             self.base_path))\n",
    "\n",
    "            # create the sample data-set\n",
    "            sample_data_set = self.__class__(sample_name, self.base_path, self.imageset_path, self.categories_path,\n",
    "                                             self.dataset_type, create_tfrecord=self.create_tfrecord)\n",
    "            # assign the converted annotations\n",
    "            sample_data_set.annotations = self.annotations\n",
    "\n",
    "            # create the data set folders\n",
    "            sample_data_set.create_folders()\n",
    "            # copy the sample images\n",
    "            sample_data_set.copy(sample_train_file_keys, sample_val_file_keys, sample_test_file_names)\n",
    "\n",
    "            self.logger.info('Finished build {} data-set containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                                int(sample * 100),\n",
    "                                                                                                self.base_path))\n",
    "\n",
    "    @classmethod\n",
    "    def get_logger(cls):\n",
    "        \"\"\"\n",
    "        Configures default logging for the system.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def split_train_val_data(data, val_size=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits the images in train and validation set\n",
    "    `data`: the data to split\n",
    "    `val_size`: the size of the validation set in percentage\n",
    "    `seed`: A random seed to reproduce splits.\n",
    "    return: the splitted train, validation images\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(data, random_state=seed, test_size=val_size) if len(data) > 1 else (data, [])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def copy_image_and_assign_orientation(source_folder, file_name, target_folder):\n",
    "    \"\"\"\n",
    "    Copy an image file from source to target and assign the EXIF metadata orientation.\n",
    "    `source_folder`: the source folder of the image file\n",
    "    `file_name`: the image file name to copy\n",
    "    `target_folder`: the target folder to copy the image file to\n",
    "    return: `True` if modified EXIF metadata could be saved, else `False`\n",
    "    \"\"\"\n",
    "\n",
    "    shutil.copy2(join(source_folder, file_name), target_folder)\n",
    "\n",
    "    # rotate image by EXIF orientation metadata and remove them\n",
    "    target_file = join(target_folder, file_name)\n",
    "    image, exif_data, rotated = assign_exif_orientation(target_file)\n",
    "    if rotated:\n",
    "        write_exif_metadata(image, exif_data, target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def input_feedback(msg, choice, choices):\n",
    "    \"\"\"\n",
    "    User input request wrapper.\n",
    "    `msg`: the message to dosplay\n",
    "    `choice`: if previous choice exist\n",
    "    `choices`: the possible choices\n",
    "    :return: the choice input\n",
    "    \"\"\"\n",
    "\n",
    "    # if decision is already made for all contents, skip feedback\n",
    "    if not (choice and choice.isupper()):\n",
    "        prompt = '{} \\n choices: {} '.format(msg, ', '.join(['{} ({})'.format(k, v) for k, v in choices.items()]))\n",
    "        while True:\n",
    "            choice = input(prompt)\n",
    "            if choice in choices:\n",
    "                break\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "    \"\"\"\n",
    "    log_memory_handler = MemoryHandler(1, flushLevel=logging_level)\n",
    "    log_memory_handler.setLevel(logging_level)\n",
    "\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setLevel(logging_level)\n",
    "\n",
    "    logger.addHandler(log_memory_handler)\n",
    "    logger.addHandler(stdout_handler)\n",
    "\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    return log_memory_handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
