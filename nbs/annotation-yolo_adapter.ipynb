{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp annotation.yolo_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import shutil\n",
    "from os.path import join, splitext, basename, isfile\n",
    "from mlcore.category_tools import read_categories\n",
    "from mlcore.core import assign_arg_prefix\n",
    "from mlcore.io.core import scan_files, create_folder\n",
    "from mlcore.image.pillow_tools import get_image_size\n",
    "from mlcore.annotation.core import Annotation, AnnotationAdapter, Region, RegionShape, SubsetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "FIELD_NAMES = ['class_number', 'c_x', 'c_y', 'width', 'height']\n",
    "DEFAULT_IMAGES_FOLDER = 'images'\n",
    "DEFAULT_IMAGE_ANNOTATIONS_FOLDER = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Annotation Adapter\n",
    "> YOLO annotation adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adapter is tested with the [YOLOv3 Pytorch](https://github.com/ultralytics/yolov3) and [YOLOv5 Pytorch](https://github.com/ultralytics/yolov5) repositories.\n",
    "Bounding boxes are normalized between [0,1].\n",
    "Images should be in a separate folder (named e.g. *images/*).\n",
    "Annotations should be in a separate folder (named e.g. *labels/*) and must have the same file name as corresponding image source file but with the ending *.txt*. The *categories.txt* file in the parent folder list all category labels which position (index) is used to label the bounding box class (see **Annotation Format** paragraph below). Below is an example structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "categories.txt\n",
    "images/\n",
    "    image1.jpg\n",
    "labels/\n",
    "    image1.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported annotations:\n",
    "- rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box values are normalized between 0 and 1.\n",
    "- Normalization Formula: ```x / image_width``` or ```y / image height``` \n",
    "- Normalization Formula: ```x * image_width``` or ```y * image height```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bounding box has the following 4 parameters:\n",
    "- Center X (bx)\n",
    "- Center Y (by)\n",
    "- Width (bw)\n",
    "- Height (bh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/yolo_bbox.png\" alt=\"YOLO-BoundingBox\" width=\"640\" caption=\"YOLO bounding box parameters.\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [A Gentle Introduction to YOLO v4 for Object detection in Ubuntu 20.04](https://robocademy.com/2020/05/01/a-gentle-introduction-to-yolo-v4-for-object-detection-in-ubuntu-20-04/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each annotation file (e.g. *image1.txt*) is a space separated CSV file where every row represents a bounding box in the image. \n",
    "A row has the following format: \n",
    "```<class_number> <center_x> <center_y> <width> <height>``` \n",
    "- *class_number*: The index of the class as listed in *categories.txt*\n",
    "- *center_x*: The normalized bounding box center x value\n",
    "- *center_y*: The normalized bounding box center y value\n",
    "- *width*: The normalized bounding box width value\n",
    "- *height*: The normalized bounding box height value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adapter has the following parameters:\n",
    "- `--path`: the path to the base folder containing the annotations (e.g.: *data/object_detection/my_collection*)\n",
    "- `--categories_file_name`: tThe path to the categories file if not set, default to *categories.txt*\n",
    "- `--images_folder_name`: the name of the folder containing the image files, if not set, default to *images*\n",
    "- `--annotations_folder_name`: The name of the folder containing the image annotations, if not set, default to *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class YOLOAnnotationAdapter(AnnotationAdapter):\n",
    "    \"\"\"\n",
    "    Adapter to read and write annotations in the YOLO format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, categories_file_name=None, images_folder_name=None, annotations_folder_name=None):\n",
    "        \"\"\"\n",
    "        YOLO Adapter to read and write annotations.\n",
    "        `path`: the folder containing the annotations\n",
    "        `categories_file_name`: the name of the categories file\n",
    "        `images_folder_name`: the name of the folder containing the image files\n",
    "        `annotations_folder_name`: the name of the folder containing the mage annotations\n",
    "        \"\"\"\n",
    "        super().__init__(path, categories_file_name)\n",
    "\n",
    "        if images_folder_name is None:\n",
    "            self.images_folder_name = DEFAULT_IMAGES_FOLDER\n",
    "        else:\n",
    "            self.images_folder_name = images_folder_name\n",
    "\n",
    "        if annotations_folder_name is None:\n",
    "            self.annotations_folder_name = DEFAULT_IMAGE_ANNOTATIONS_FOLDER\n",
    "        else:\n",
    "            self.annotations_folder_name = annotations_folder_name\n",
    "\n",
    "    @classmethod\n",
    "    def argparse(cls, prefix=None):\n",
    "        \"\"\"\n",
    "        Returns the argument parser containing argument definition for command line use.\n",
    "        `prefix`: a parameter prefix to set, if needed\n",
    "        return: the argument parser\n",
    "        \"\"\"\n",
    "        parser = super(YOLOAnnotationAdapter, cls).argparse(prefix=prefix)\n",
    "        parser.add_argument(assign_arg_prefix('--images_folder_name', prefix),\n",
    "                            dest=\"images_folder_name\",\n",
    "                            help=\"The name of the folder containing the image files.\",\n",
    "                            default=None)\n",
    "        parser.add_argument(assign_arg_prefix('--annotations_folder_name', prefix),\n",
    "                            dest=\"annotations_folder_name\",\n",
    "                            help=\"The name of the folder containing the mage annotations.\",\n",
    "                            default=None)\n",
    "        return parser\n",
    "\n",
    "    def read_annotations(self, subset_type=SubsetType.TRAINVAL):\n",
    "        \"\"\"\n",
    "        Reads YOLO annotations.\n",
    "        `subset_type`: the subset type to read\n",
    "        return: the annotations as dictionary\n",
    "        \"\"\"\n",
    "        path = join(self.path, str(subset_type))\n",
    "        annotations = {}\n",
    "        annotations_path = join(path, self.annotations_folder_name)\n",
    "        images_path = join(path, self.images_folder_name)\n",
    "        logger.info('Read images from {}'.format(images_path))\n",
    "        logger.info('Read annotations from {}'.format(annotations_path))\n",
    "\n",
    "        annotation_files = scan_files(annotations_path, file_extensions='.txt')\n",
    "        categories = read_categories(join(self.path, self.categories_file_name))\n",
    "        categories_len = len(categories)\n",
    "        skipped_annotations = []\n",
    "\n",
    "        for annotation_file in annotation_files:\n",
    "            with open(annotation_file, newline='') as csv_file:\n",
    "                annotation_file_name = basename(annotation_file)\n",
    "                file_name, _ = splitext(annotation_file_name)\n",
    "                file_path = join(images_path, '{}{}'.format(file_name, '.jpg'))\n",
    "\n",
    "                if not isfile(file_path):\n",
    "                    logger.warning(\"{}: Source file not found, skip annotation.\".format(file_path))\n",
    "                    skipped_annotations.append(file_path)\n",
    "                    continue\n",
    "\n",
    "                if annotation_file not in annotations:\n",
    "                    annotations[annotation_file] = Annotation(annotation_id=annotation_file, file_path=file_path)\n",
    "\n",
    "                annotation = annotations[annotation_file]\n",
    "\n",
    "                reader = csv.DictReader(csv_file, fieldnames=FIELD_NAMES, delimiter=' ')\n",
    "                _, image_width, image_height = get_image_size(file_path)\n",
    "                for row in reader:\n",
    "                    c_x = float(row[\"c_x\"])\n",
    "                    c_y = float(row[\"c_y\"])\n",
    "                    width = float(row[\"width\"])\n",
    "                    height = float(row[\"height\"])\n",
    "                    class_number = int(row[\"class_number\"])\n",
    "                    # denormalize bounding box\n",
    "                    x_min = self._denormalize_value(c_x - (width / 2), image_width)\n",
    "                    y_min = self._denormalize_value(c_y - (height / 2), image_height)\n",
    "                    x_max = x_min + self._denormalize_value(width, image_width)\n",
    "                    y_max = y_min + self._denormalize_value(height, image_height)\n",
    "                    points_x = [x_min, x_max]\n",
    "                    points_y = [y_min, y_max]\n",
    "\n",
    "                    labels = [categories[class_number]] if class_number < categories_len else []\n",
    "                    if not labels:\n",
    "                        logger.warning(\"{}: Class number exceeds categories, set label as empty.\".format(\n",
    "                            annotation_file\n",
    "                        ))\n",
    "                    region = Region(shape=RegionShape.RECTANGLE, points_x=points_x, points_y=points_y, labels=labels)\n",
    "                    annotation.regions.append(region)\n",
    "\n",
    "        logger.info('Finished read annotations')\n",
    "        logger.info('Annotations read: {}'.format(len(annotations)))\n",
    "        if skipped_annotations:\n",
    "            logger.info('Annotations skipped: {}'.format(len(skipped_annotations)))\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def write_annotations(self, annotations, subset_type=SubsetType.TRAINVAL):\n",
    "        \"\"\"\n",
    "        Writes YOLO annotations to the annotations folder and copy the corresponding source files.\n",
    "        `annotations`: the annotations as dictionary\n",
    "        `subset_type`: the subset type to write\n",
    "        return: a list of written target file paths\n",
    "        \"\"\"\n",
    "        path = join(self.path, str(subset_type))\n",
    "        create_folder(path)\n",
    "        annotations_path = join(path, self.annotations_folder_name)\n",
    "        annotations_folder = create_folder(annotations_path)\n",
    "        images_path = join(path, self.images_folder_name)\n",
    "        images_folder = create_folder(images_path)\n",
    "        categories = read_categories(join(self.path, self.categories_file_name))\n",
    "\n",
    "        logger.info('Write images to {}'.format(images_folder))\n",
    "        logger.info('Write annotations to {}'.format(annotations_folder))\n",
    "\n",
    "        copied_files = []\n",
    "        skipped_annotations = []\n",
    "\n",
    "        for annotation in annotations.values():\n",
    "            annotation_file_name = basename(annotation.file_path)\n",
    "            file_name, _ = splitext(annotation_file_name)\n",
    "            annotations_file = join(annotations_folder, '{}{}'.format(file_name, '.txt'))\n",
    "            target_file = join(images_folder, annotation_file_name)\n",
    "\n",
    "            if not isfile(annotation.file_path):\n",
    "                logger.warning(\"{}: Source file not found, skip annotation.\".format(annotation.file_path))\n",
    "                skipped_annotations.append(annotation.file_path)\n",
    "                continue\n",
    "            if isfile(target_file):\n",
    "                logger.warning(\"{}: Target file already exist, skip annotation.\".format(annotation.file_path))\n",
    "                skipped_annotations.append(annotation.file_path)\n",
    "                continue\n",
    "\n",
    "            _, image_width, image_height = get_image_size(annotation.file_path)\n",
    "            rows = []\n",
    "            skipped_regions = []\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                if region.shape != RegionShape.RECTANGLE:\n",
    "                    logger.warning('Unsupported shape {}, skip region {} at path: {}'.format(region.shape,\n",
    "                                                                                             index,\n",
    "                                                                                             annotations_file))\n",
    "                    skipped_regions.append(region)\n",
    "                    continue\n",
    "\n",
    "                x_min, x_max = region.points_x\n",
    "                y_min, y_max = region.points_y\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                # normalize bounding box\n",
    "                c_x = self._normalize_value(x_min + width / 2, image_width)\n",
    "                c_y = self._normalize_value(y_min + height / 2, image_height)\n",
    "                width = self._normalize_value(width, image_width)\n",
    "                height = self._normalize_value(height, image_height)\n",
    "                label = region.labels[0] if len(region.labels) else ''\n",
    "                try:\n",
    "                    class_number = categories.index(label)\n",
    "                except ValueError:\n",
    "                    logger.warning('Unsupported label {}, skip region {} at path: {}'.format(label,\n",
    "                                                                                             index,\n",
    "                                                                                             annotations_file))\n",
    "                    skipped_regions.append(region)\n",
    "                    continue\n",
    "                rows.append(dict(zip(FIELD_NAMES, [class_number, c_x, c_y, width, height])))\n",
    "\n",
    "            if len(skipped_regions) == len(annotation.regions):\n",
    "                logger.warning(\"{}: All regions skipped, skip annotation.\".format(annotation.file_path))\n",
    "                skipped_annotations.append(annotation.file_path)\n",
    "                continue\n",
    "\n",
    "            # write the annotations\n",
    "            with open(annotations_file, 'w', newline='') as csv_file:\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=FIELD_NAMES, delimiter=' ')\n",
    "                writer.writerows(rows)\n",
    "\n",
    "            # copy the file\n",
    "            shutil.copy2(annotation.file_path, target_file)\n",
    "            copied_files.append(target_file)\n",
    "\n",
    "        logger.info('Finished write annotations')\n",
    "        logger.info('Annotations written: {}'.format(len(annotations) - len(skipped_annotations)))\n",
    "        if skipped_annotations:\n",
    "            logger.info('Annotations skipped: {}'.format(len(skipped_annotations)))\n",
    "        return copied_files\n",
    "\n",
    "    @classmethod\n",
    "    def _denormalize_value(cls, value, metric):\n",
    "        \"\"\"\n",
    "        Denormalize a bounding box value\n",
    "        `value`: the value to denormalize\n",
    "        `metric`: the metric to denormalize from\n",
    "        return: the denormalized value\n",
    "        \"\"\"\n",
    "        return int(value * metric)\n",
    "\n",
    "    @classmethod\n",
    "    def _normalize_value(cls, value, metric):\n",
    "        \"\"\"\n",
    "        Normalize a bounding box value\n",
    "        `value`: the value to normalize\n",
    "        `metric`: the metric to normalize against\n",
    "        return: the normalized value\n",
    "        \"\"\"\n",
    "        return float(value) / metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(YOLOAnnotationAdapter.list_files)\n",
    "show_doc(YOLOAnnotationAdapter.read_annotations)\n",
    "show_doc(YOLOAnnotationAdapter.read_categories)\n",
    "show_doc(YOLOAnnotationAdapter.write_files)\n",
    "show_doc(YOLOAnnotationAdapter.write_annotations)\n",
    "show_doc(YOLOAnnotationAdapter.write_categories)\n",
    "show_doc(YOLOAnnotationAdapter.argparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
