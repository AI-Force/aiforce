{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tensorflow.tfrecord_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import logging\n",
    "import logging.handlers\n",
    "import argparse\n",
    "import io\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from os import environ\n",
    "from os.path import join\n",
    "from mlcore.core import Type, infer_type\n",
    "from mlcore.annotation.via import read_annotations\n",
    "from mlcore.image.pillow_tools import get_image_size\n",
    "from mlcore import category_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow TFRecord Builder\n",
    "> Creates TFRecord Files and Labelmap protobuffer text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_tfrecord_entry(categories, file_annotation):\n",
    "    \"\"\"\n",
    "    Create tfrecord entry with annotations for one file / image.\n",
    "    `categories`: the categories used\n",
    "    `file_annotation`: the annotation of a file / image\n",
    "    return: the tfrecord entry\n",
    "    \"\"\"\n",
    "    with tf.io.gfile.GFile(file_annotation.file_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    _, width, height = get_image_size(io.BytesIO(encoded_jpg))\n",
    "\n",
    "    file_name = file_annotation.file_name.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, annotation in enumerate(file_annotation.annotations):\n",
    "        x_min = min(annotation.points_x)\n",
    "        y_min = min(annotation.points_y)\n",
    "        x_max = max(annotation.points_x)\n",
    "        y_max = max(annotation.points_y)\n",
    "        category = annotation.labels[0] if len(annotation.labels) else ''\n",
    "\n",
    "        xmins.append(x_min / width)\n",
    "        xmaxs.append(x_max / width)\n",
    "        ymins.append(y_min / height)\n",
    "        ymaxs.append(y_max / height)\n",
    "        classes_text.append(category.encode('utf8'))\n",
    "        classes.append(categories.index(category))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': int64_feature(height),\n",
    "        'image/width': int64_feature(width),\n",
    "        'image/filename': bytes_feature(file_name),\n",
    "        'image/source_id': bytes_feature(file_name),\n",
    "        'image/encoded': bytes_feature(encoded_jpg),\n",
    "        'image/format': bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': float_list_feature(ymaxs),\n",
    "        'image/object/class/text': bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_tfrecord_file(output_path, categories, annotations):\n",
    "    \"\"\"\n",
    "    Create a tfrecord file for a sub-data-set, which can be one of the following: training, validation, test\n",
    "    `output_path`: the path including the filename of the tfrecord file\n",
    "    `categories`: the categories used\n",
    "    `annotations`: the annotations of the files / images\n",
    "    \"\"\"\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    for annotation in annotations.values():\n",
    "        tf_example = create_tfrecord_entry(categories, annotation)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "    logger.info('Successfully created the TFRecord file: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for creating TFRecord data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Labelmap Protobuffer Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_labelmap_file(output_path, categories, start=1):\n",
    "    \"\"\"\n",
    "    Create labelmap protobuffer text file containing the categories.\n",
    "    Format is compatible with Tensorflow Object Detection API.\n",
    "    For object detection data-sets, the categories should exclude the background class and `start` should be 1.\n",
    "    `output_path`: the path including the filename of the protobuffer text file\n",
    "    `categories`: a list of the categories to write\n",
    "    `start`: the category index for the first category\n",
    "    \"\"\"\n",
    "    # create label_map data\n",
    "    label_map = ''\n",
    "    for index, category in enumerate(categories, start=start):\n",
    "        label_map = label_map + \"item {\\n\"\n",
    "        label_map = label_map + \" id: \" + str(index) + \"\\n\"\n",
    "        label_map = label_map + \" name: '\" + category + \"'\\n}\\n\\n\"\n",
    "    label_map = label_map[:-1]\n",
    "\n",
    "    # write label_map file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(label_map)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "\n",
    "    :param logging_level: The logging level to use.\n",
    "    \"\"\"\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(logging_level)\n",
    "\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the image size calculator from command line, use the following command:\n",
    "`python -m mlcore.tensorflow.tfrecord_builder [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `--output`: The path to store the TFRecord file. (e.g.:  *datasets/image_object_detection/car_damage/train/train.record*)\n",
    "- `--source`: The path to The path to the data-set source files. (e.g.:  *datasets/image_object_detection/car_damage/train*)\n",
    "- `--categories`: The path to the data-set categories file. (e.g.: *datasets/image_object_detection/car_damage/categories.txt*)\n",
    "- `--type`: The type of the data-set, if not explicitly set try to infer from categories file path.\n",
    "- `--annotation`: The path to the image-set annotation file, the data-set is build from. (e.g.: *datasets/image_object_detection/car_damage/train/via_region_data.json*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    configure_logging()\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-o\",\n",
    "                        \"--output\",\n",
    "                        help=\"Path of output TFRecord (.record) file.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"-s\",\n",
    "                        \"--source\",\n",
    "                        help=\"The path to the data-set source files.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"-c\",\n",
    "                        \"--categories\",\n",
    "                        help=\"The path to the data-set categories file.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"-a\",\n",
    "                        \"--annotation\",\n",
    "                        help=\"The path to the data-set annotation file, the data-set is build from.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"-t\",\n",
    "                        \"--type\",\n",
    "                        help=\"The type of the data-set, if not explicitly set try to infer from categories file path.\",\n",
    "                        choices=list(Type),\n",
    "                        type=Type,\n",
    "                        default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    category_file_path = args.categories\n",
    "    data_set_type = args.type\n",
    "    # try to infer the data-set type if not explicitly set\n",
    "    if data_set_type is None:\n",
    "        try:\n",
    "            data_set_type = infer_type(category_file_path)\n",
    "        except ValueError as e:\n",
    "            logger.error(e)\n",
    "            sys.exit(1)\n",
    "\n",
    "    categories = category_tools.read_categories(category_file_path, data_set_type)\n",
    "    annotations = read_annotations(args.annotation, args.source)\n",
    "\n",
    "    create_tfrecord_file(args.output, categories, annotations)\n",
    "\n",
    "    logger.info('FINISHED!!!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
