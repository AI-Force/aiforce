{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.image_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import logging\n",
    "from os.path import join, isfile, splitext, basename\n",
    "from functools import partial\n",
    "from mlcore.core import assign_arg_prefix, input_feedback\n",
    "from mlcore.annotation.core import AnnotationAdapter\n",
    "from mlcore.dataset.image_object_detection import ImageObjectDetectionDataset\n",
    "from mlcore.image.pillow_tools import assign_exif_orientation, get_image_size, write_mask\n",
    "from mlcore.io.core import create_folder\n",
    "from mlcore.annotation.core import RegionShape, convert_region, region_bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for image segmentation\n",
    "\n",
    "> Creates a dataset for image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data-set for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The data-set is created based on an image-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-sets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[image_set_type]*/*[image_set_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[image_set_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the image-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Set Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[data_set_type]*/*[data_set_name]*\n",
    "where `[data_set_type]` is the same as the corresponding `[image_set_type]` and `[data_set_name]` is the same as the corresponding `[image_set_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[data_set_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the data-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a segmentation data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation data-set can be created from a segmentation image-set.\n",
    "All images are validated against the annotations, if they contain at least one annotation and that the annotation category belongs to one of the given categories. The annotations have to be in [VIA v1](http://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.5.html) json format. Rectangle annotations are converted into polygon annotations for unique segment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageSegmentationDataset(ImageObjectDetectionDataset):\n",
    "\n",
    "    SEMANTIC_MASK_FOLDER = 'semantic_masks'\n",
    "\n",
    "    def __init__(self, input_adapter: AnnotationAdapter, output_adapter: AnnotationAdapter, split=None, seed=None,\n",
    "                 sample=None, tfrecord=False, join_overlapping_regions=False, annotation_area_threshold=None,\n",
    "                 generate_semantic_masks=True):\n",
    "        super().__init__(input_adapter, output_adapter, split, seed, sample, tfrecord, join_overlapping_regions,\n",
    "                         annotation_area_threshold)\n",
    "        self.generate_semantic_masks = generate_semantic_masks\n",
    "        self.semantic_mask_folder = join(self.output_adapter.path, self.SEMANTIC_MASK_FOLDER)\n",
    "\n",
    "    @classmethod\n",
    "    def argparse(cls, prefix=None):\n",
    "        \"\"\"\n",
    "        Returns the argument parser containing argument definition for command line use.\n",
    "        `prefix`: a parameter prefix to set, if needed\n",
    "        return: the argument parser\n",
    "        \"\"\"\n",
    "        parser = super(ImageSegmentationDataset, cls).argparse(prefix=prefix)\n",
    "        parser.add_argument(assign_arg_prefix(\"--generate_semantic_masks\", prefix),\n",
    "                            dest=\"generate_semantic_masks\",\n",
    "                            help=\"Whether semantic masks should be generated.\",\n",
    "                            action=\"store_true\",\n",
    "                            default=True)\n",
    "        return parser\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"\n",
    "        Creates the data-set folder structure, if not exist\n",
    "        \"\"\"\n",
    "        super().create_folders()\n",
    "\n",
    "        if self.generate_semantic_masks:\n",
    "            # create semantic mask file folder and remove previous data if exist\n",
    "            semantic_mask_folder = create_folder(self.semantic_mask_folder, clear=True)\n",
    "            logger.info(\"Created semantic mask folder {}\".format(semantic_mask_folder))\n",
    "\n",
    "    def copy(self, train_annotation_keys, val_annotation_keys, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the dataset and remove EXIF orientation information by hard-rotate the images.\n",
    "        If tfrecords should be build, create tfrecords for train and val subsets and generate a labelmap.pbtxt file.\n",
    "        If semantic masks should be generate, masks for train and val subsets are build.\n",
    "        `train_annotation_keys`: The list of training annotation keys\n",
    "        `val_annotation_keys`: The list of validation annotation keys\n",
    "        `test_files`: The list of test file paths\n",
    "        return: A tuple containing train, val and test target file paths\n",
    "        \"\"\"\n",
    "\n",
    "        train_targets, val_targets, test_targets = super().copy(train_annotation_keys, val_annotation_keys, test_files)\n",
    "\n",
    "        if self.generate_semantic_masks:\n",
    "            # save semantic masks\n",
    "            self._save_semantic_masks(train_annotation_keys + val_annotation_keys)\n",
    "\n",
    "        return train_targets, val_targets, test_targets\n",
    "\n",
    "    def convert_annotations(self):\n",
    "        \"\"\"\n",
    "        Converts segmentation regions from rectangle to polygon, if exist\n",
    "        \"\"\"\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'position',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    't': 'Trim',  # transform the annotation\n",
    "                    'T': 'Trim All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: p_min < 0 or p_max >= size,\n",
    "                'message': '{} -> {} : {}Exceeds image {}. \\n Points \\n x: {} \\n y: {}',\n",
    "                'transform': lambda p, size=0: max(min(p, size - 1), 0),\n",
    "            },\n",
    "            {\n",
    "                'name': 'size',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, _: p_max - p_min <= 1,\n",
    "                'message': '{} -> {} : {}Shape {} is <= 1 pixel. \\n Points \\n x: {} \\n y: {}',\n",
    "                'transform': lambda p, size=0: p,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        logger.info('Start convert image annotations from {}'.format(self.input_adapter.path))\n",
    "\n",
    "        for annotation in self.annotations.values():\n",
    "            # skip file, if regions are empty or file do not exist\n",
    "            if not (annotation.regions and isfile(annotation.file_path)):\n",
    "                continue\n",
    "\n",
    "            image, _, __ = assign_exif_orientation(annotation.file_path)\n",
    "            image_width, image_height = image.size\n",
    "\n",
    "            delete_regions = {}\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                # convert from rect to polygon if needed\n",
    "                convert_region(region, RegionShape.POLYGON)\n",
    "\n",
    "                for step in steps:\n",
    "                    # validate the shape size\n",
    "                    (x_min, x_max), (y_min, y_max) = region_bounding_box(region)\n",
    "\n",
    "                    width_condition = step['condition'](x_min, x_max, image_width)\n",
    "                    height_condition = step['condition'](y_min, y_max, image_height)\n",
    "                    if width_condition or height_condition:\n",
    "                        size_message = ['width'] if width_condition else []\n",
    "                        size_message.extend(['height'] if height_condition else [])\n",
    "                        message = step['message'].format(annotation.file_path, index, ' ', ' and '.join(size_message),\n",
    "                                                         region.points_x, region.points_y)\n",
    "\n",
    "                        step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                        choice_op = step['choice'].lower()\n",
    "                        # if skip the shapes\n",
    "                        if choice_op == 's':\n",
    "                            delete_regions[index] = True\n",
    "                            message = step['message'].format(annotation.file_path, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             region.points_x, region.points_y)\n",
    "                            logger.info(message)\n",
    "\n",
    "                            break\n",
    "                        else:\n",
    "                            region.points_x = list(map(partial(step['transform'], size=image_width), region.points_x))\n",
    "                            region.points_y = list(map(partial(step['transform'], size=image_height), region.points_y))\n",
    "\n",
    "                            message = step['message'].format(annotation.file_path, index,\n",
    "                                                             '{} '.format(step['choices'][choice_op]),\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             region.points_x, region.points_y)\n",
    "                            logger.info(message)\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for index in sorted(list(delete_regions.keys()), reverse=True):\n",
    "                del annotation.regions[index]\n",
    "\n",
    "        print('Finished convert image annotations from {}'.format(self.input_adapter.path))\n",
    "\n",
    "    def _save_semantic_masks(self, annotation_keys):\n",
    "        \"\"\"\n",
    "        Create semantic segmentation mask png files out of the annotations.\n",
    "        The mask file name is the same as the image file name but is stored in png format.\n",
    "        `annotation_keys`: The annotation keys to create the segmentation masks for\n",
    "        \"\"\"\n",
    "        from skimage import draw\n",
    "\n",
    "        num_masks = len(annotation_keys)\n",
    "        logger.info('Start create {} segmentation masks in {}'.format(num_masks, self.semantic_mask_folder))\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        for index, key in enumerate(annotation_keys):\n",
    "            annotation = self.annotations[key]\n",
    "\n",
    "            if not annotation.regions:\n",
    "                continue\n",
    "\n",
    "            image, image_width, image_height = get_image_size(annotation.file_path)\n",
    "\n",
    "            # Convert polygons to a bitmap mask of shape\n",
    "            # [height, width]\n",
    "            mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "\n",
    "            # sort the regions by category priority for handling pixels which are assigned to more than one category\n",
    "            # the category with higher index paint over the category with lower index\n",
    "            for region in sorted(annotation.regions, key=lambda r: self.categories.index(r.labels[0])):\n",
    "                class_id = self.categories.index(region.labels[0]) + 1\n",
    "\n",
    "                # Get indexes of pixels inside the polygon and set them to 1\n",
    "                rr, cc = draw.polygon(region.points_y, region.points_x)\n",
    "                mask[rr, cc] = class_id\n",
    "\n",
    "            # save the semantic mask\n",
    "            file_name = basename(annotation.file_path)\n",
    "            mask_path = join(self.semantic_mask_folder, splitext(file_name)[0] + '.png')\n",
    "            write_mask(mask, mask_path)\n",
    "\n",
    "            logger.info('{} / {} - Created segmentation mask {}'.format(index + 1, num_masks, mask_path))\n",
    "\n",
    "        logger.info('Finish create {} segmentation masks in {}'.format(num_masks, self.semantic_mask_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
