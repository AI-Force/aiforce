{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.image_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from os.path import join, dirname, basename\n",
    "from datetime import datetime\n",
    "from mlcore.annotation.folder_category_adapter import read_annotations as folder_category_read_annotations\n",
    "from mlcore.annotation.multi_category_adapter import read_annotations as multi_category_read_annotations\n",
    "from mlcore.annotation.multi_category_adapter import write_annotations as multi_category_write_annotations\n",
    "from mlcore.dataset.core import Dataset, configure_logging\n",
    "from mlcore.dataset.type import DatasetType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE = 'annotations.csv'\n",
    "DEFAULT_SPLIT = 0.2\n",
    "DATA_SET_FOLDER = 'datasets'\n",
    "DATASET_TYPE = DatasetType.IMAGE_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for image classification\n",
    "\n",
    "> Creates a dataset for image classification. Single and multi label classification is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data-set for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The data-set is created based on an image-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-sets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[image_set_type]*/*[image_set_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[image_set_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the image-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Set Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[data_set_type]*/*[data_set_name]*\n",
    "where `[data_set_type]` is the same as the corresponding `[image_set_type]` and `[data_set_name]` is the same as the corresponding `[image_set_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[data_set_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the data-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification data-set can be created from a classification image-set. \n",
    "All images are validated, if they belong to one of the given categories. If categories with no images are found or images belong to a category not listed in `categories.txt`, the data-set can not be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classification dataset.\n",
    "    `name`: The name of the dataset.\n",
    "    `base_path`: The dataset base-path.\n",
    "    `imageset_path`: The imageset source path.\n",
    "    `categories_path`: The path to the categories.txt file.\n",
    "    `annotations_path`: The path to the annotations-file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, base_path, imageset_path, categories_path, annotations_path=None, create_tfrecord=False):\n",
    "        super().__init__(name, base_path, imageset_path, categories_path, DATASET_TYPE, create_tfrecord)\n",
    "        self.annotations_path = annotations_path\n",
    "        # if no annotation file exist, generate annotations based on the folder names\n",
    "        if not annotations_path:\n",
    "            self.annotations = folder_category_read_annotations(self.train_val_folder)\n",
    "        else:\n",
    "            self.annotations = multi_category_read_annotations(annotations_path, self.train_val_folder)\n",
    "\n",
    "    def copy(self, train_file_keys, val_file_keys, test_file_names=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set, generate the annotations for train and val images.\n",
    "        `train_file_keys`: The list of training image keys\n",
    "        `val_file_keys`: The list of validation image keys\n",
    "        `test_file_names`: The list of test image file names\n",
    "        return: A tuple containing train and val annotations\n",
    "        \"\"\"\n",
    "\n",
    "        annotations_train, annotations_val = super().copy(train_file_keys, val_file_keys, test_file_names)\n",
    "\n",
    "        # write the split train annotations\n",
    "        if annotations_train:\n",
    "            annotations_target_path = join(self.train_folder, DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            multi_category_write_annotations(annotations_target_path, annotations_train)\n",
    "\n",
    "        # write the split val annotations\n",
    "        if annotations_val:\n",
    "            annotations_target_path = join(self.val_folder, DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            multi_category_write_annotations(annotations_target_path, annotations_val)\n",
    "\n",
    "        return annotations_train, annotations_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data-set from an image-set. Handles currently classification and segmentation image-sets taken from the image-set-type, which is the parent folder, the image-set folder is located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def build_dataset(category_file_path, output, annotation_file_path=None, split=DEFAULT_SPLIT, seed=None, sample=0,\n",
    "                  create_tfrecord=False, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Build the dataset for training, Validation and test\n",
    "    `category_file_path`: the filename of the categories file\n",
    "    `output`: the dataset base folder to build the dataset in\n",
    "    `annotation_file_path`: the file path to the annotation file\n",
    "    `split`: the size of the validation set as percentage\n",
    "    `seed`: random seed to reproduce splits\n",
    "    `sample`: the size of the sample set as percentage\n",
    "    `create_tfrecord`: Also create .tfrecord files.\n",
    "    `dataset_name`: the name of the dataset, if not set infer from the category file path\n",
    "    \"\"\"\n",
    "    log_memory_handler = configure_logging()\n",
    "\n",
    "    path = dirname(category_file_path)\n",
    "\n",
    "    # try to infer the data-set name if not explicitly set\n",
    "    if dataset_name is None:\n",
    "        dataset_name = basename(path)\n",
    "\n",
    "    logger.info('Build parameters:')\n",
    "    logger.info(' '.join(sys.argv[1:]))\n",
    "    logger.info('Build configuration:')\n",
    "    logger.info('category_file_path: {}'.format(category_file_path))\n",
    "    logger.info('annotation_file_path: {}'.format(annotation_file_path))\n",
    "    logger.info('create_tfrecord: {}'.format(create_tfrecord))\n",
    "    logger.info('split: {}'.format(split))\n",
    "    logger.info('seed: {}'.format(seed))\n",
    "    logger.info('sample: {}'.format(sample))\n",
    "    logger.info('dataset_type: {}'.format(DATASET_TYPE))\n",
    "    logger.info('output: {}'.format(output))\n",
    "    logger.info('name: {}'.format(dataset_name))\n",
    "\n",
    "    logger.info('Start build {} dataset {} at {}'.format(DATASET_TYPE, dataset_name, output))\n",
    "\n",
    "    dataset = ImageClassificationDataset(dataset_name, output, path, category_file_path, annotation_file_path)\n",
    "\n",
    "    # create the dataset folders\n",
    "    logger.info(\"Start create the dataset folders at {}\".format(dataset.base_path))\n",
    "    dataset.create_folders()\n",
    "    logger.info(\"Finished create the dataset folders at {}\".format(dataset.base_path))\n",
    "\n",
    "    # create the build log file\n",
    "    log_file_name = datetime.now().strftime(\"build_%Y.%m.%d-%H.%M.%S.log\")\n",
    "    file_handler = logging.FileHandler(join(dataset.folder, log_file_name), encoding=\"utf-8\")\n",
    "    log_memory_handler.setTarget(file_handler)\n",
    "\n",
    "    # build the dataset\n",
    "    dataset.build(split, seed, sample)\n",
    "\n",
    "    logger.info('Finished build {} dataset {} at {}'.format(DATASET_TYPE, dataset_name, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the dataset builder from command line, use the following command:\n",
    "`python -m mlcore.dataset.image_classification [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `[categories]`: The path to the categories file. (e.g.: *categories.txt*)\n",
    "- `--annotation`: The path to the image-set annotation file, the dataset is build from. (e.g.: *imagesets/classification/car_damage/annotations.csv* for classification, *imagesets/segmentation/car_damage/via_region_data.json* for segmentation)\n",
    "- `--split`: The percentage of the data which belongs to validation set, default to *0.2* (=20%)\n",
    "- `--seed`: A random seed to reproduce splits, default to None\n",
    "- `--sample`: The percentage of the data which will be copied as a sample set with in a separate folder with \"_sample\" suffix. If not set, no sample dataset will be created.\n",
    "- `--tfrecord`: Also create .tfrecord files.\n",
    "- `--output`: The path of the dataset folder, default to *../datasets*.\n",
    "- `--name`: The name of the dataset, if not explicitly set try to infer from categories file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"categories\",\n",
    "                        help=\"The path to the imageset categories file.\")\n",
    "    parser.add_argument(\"--annotation\",\n",
    "                        help=\"The path to the imageset annotation file, the dataset is build from.\",\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--split\",\n",
    "                        help=\"Percentage of the data which belongs to validation set.\",\n",
    "                        type=float,\n",
    "                        default=0.2)\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        help=\"A random seed to reproduce splits.\",\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--sample\",\n",
    "                        help=\"Percentage of the data which will be copied as a sample set.\",\n",
    "                        type=float,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--tfrecord\",\n",
    "                        help=\"Also create .tfrecord files.\",\n",
    "                        action=\"store_true\")\n",
    "    parser.add_argument(\"--output\",\n",
    "                        help=\"The path of the dataset folder.\",\n",
    "                        default=DATA_SET_FOLDER)\n",
    "    parser.add_argument(\"--name\",\n",
    "                        help=\"The name of the dataset, if not explicitly set try to infer from categories file path.\",\n",
    "                        default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    build_dataset(args.categories, args.output, args.annotation, args.split, args.seed, args.sample, args.tfrecord,\n",
    "                  args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
