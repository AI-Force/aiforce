{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from os.path import join, dirname, basename\n",
    "from datetime import datetime\n",
    "from logging.handlers import MemoryHandler\n",
    "from mlcore.dataset.type import DatasetType, infer_dataset_type\n",
    "from mlcore.dataset.image_classification import ImageClassificationDataset\n",
    "from mlcore.dataset.image_object_detection import ImageObjectDetectionDataset\n",
    "from mlcore.dataset.image_segmentation import ImageSegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "CATEGORY_LABEL_KEY = 'category'\n",
    "DEFAULT_SPLIT = 0.2\n",
    "DATASET_FOLDER = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generator\n",
    "\n",
    "> Dataset generator Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The dataset is created based on an imageset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imageset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagesets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[imageset_type]*/*[imageset_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[imageset_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the imageset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[dataset_type]*/*[dataset_name]*\n",
    "where `[dataset_type]` is the same as the corresponding `[imageset_type]` and `[dataset_name]` is the same as the corresponding `[imageset_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[dataset_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the dataset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "    \"\"\"\n",
    "    log_memory_handler = MemoryHandler(1, flushLevel=logging_level)\n",
    "    log_memory_handler.setLevel(logging_level)\n",
    "\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setLevel(logging_level)\n",
    "\n",
    "    logger.addHandler(log_memory_handler)\n",
    "    logger.addHandler(stdout_handler)\n",
    "\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    return log_memory_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data-set from an image-set. Handles currently classification and segmentation image-sets taken from the image-set-type, which is the parent folder, the image-set folder is located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def build_dataset(category_file_path, output, annotation_file_path=None, split=DEFAULT_SPLIT, seed=None, sample=0,\n",
    "                  dataset_type=None, create_tfrecord=False, join_overlapping_regions=False,\n",
    "                  annotation_area_threshold=None, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Build the dataset for training, Validation and test\n",
    "    `category_file_path`: the filename of the categories file\n",
    "    `output`: the dataset base folder to build the dataset in\n",
    "    `annotation_file_path`: the file path to the annotation file\n",
    "    `split`: the size of the validation set as percentage\n",
    "    `seed`: random seed to reproduce splits\n",
    "    `sample`: the size of the sample set as percentage\n",
    "    `dataset_type`: the type of the dataset, if not set infer from the category file path\n",
    "    `create_tfrecord`: Also create .tfrecord files.\n",
    "    `join_overlapping_regions`: Whether overlapping regions of same category should be joined.\n",
    "    `annotation_area_threshold`: Keep only annotations with minimum size (width or height) related to image size\n",
    "    `dataset_name`: the name of the dataset, if not set infer from the category file path\n",
    "    \"\"\"\n",
    "    log_memory_handler = configure_logging()\n",
    "\n",
    "    # try to infer the dataset type if not explicitly set\n",
    "    if dataset_type is None:\n",
    "        try:\n",
    "            dataset_type = infer_dataset_type(category_file_path)\n",
    "        except ValueError as e:\n",
    "            logger.error(e)\n",
    "            return\n",
    "\n",
    "    path = dirname(category_file_path)\n",
    "\n",
    "    # try to infer the dataset name if not explicitly set\n",
    "    if dataset_name is None:\n",
    "        dataset_name = basename(path)\n",
    "\n",
    "    logger.info('Build parameters:')\n",
    "    logger.info(' '.join(sys.argv[1:]))\n",
    "    logger.info('Build configuration:')\n",
    "    logger.info('category_file_path: {}'.format(category_file_path))\n",
    "    logger.info('annotation_file_path: {}'.format(annotation_file_path))\n",
    "    logger.info('split: {}'.format(split))\n",
    "    logger.info('seed: {}'.format(seed))\n",
    "    logger.info('sample: {}'.format(sample))\n",
    "    logger.info('type: {}'.format(dataset_type))\n",
    "    logger.info('output: {}'.format(output))\n",
    "    logger.info('join_overlapping_regions: {}'.format(join_overlapping_regions))\n",
    "    logger.info('annotation_area_threshold: {}'.format(annotation_area_threshold))\n",
    "    logger.info('name: {}'.format(dataset_name))\n",
    "\n",
    "    dataset = None\n",
    "    logger.info('Start build {} dataset {} at {}'.format(dataset_type, dataset_name, output))\n",
    "\n",
    "    if dataset_type == DatasetType.IMAGE_CLASSIFICATION:\n",
    "        dataset = ImageClassificationDataset(dataset_name, output, path, category_file_path, annotation_file_path)\n",
    "    elif dataset_type == DatasetType.IMAGE_SEGMENTATION:\n",
    "        dataset = ImageSegmentationDataset(dataset_name, output, path, category_file_path, annotation_file_path)\n",
    "    elif dataset_type == DatasetType.IMAGE_OBJECT_DETECTION:\n",
    "        dataset = ImageObjectDetectionDataset(dataset_name, output, path, category_file_path, annotation_file_path,\n",
    "                                              create_tfrecord, join_overlapping_regions, annotation_area_threshold)\n",
    "\n",
    "    if dataset:\n",
    "        # create the dataset folders\n",
    "        logger.info(\"Start create the dataset folders at {}\".format(dataset.base_path))\n",
    "        dataset.create_folders()\n",
    "        logger.info(\"Finished create the dataset folders at {}\".format(dataset.base_path))\n",
    "\n",
    "        # create the build log file\n",
    "        log_file_name = datetime.now().strftime(\"build_%Y.%m.%d-%H.%M.%S.log\")\n",
    "        file_handler = logging.FileHandler(join(dataset.folder, log_file_name), encoding=\"utf-8\")\n",
    "        log_memory_handler.setTarget(file_handler)\n",
    "\n",
    "        # build the dataset\n",
    "        dataset.build(split, seed, sample)\n",
    "\n",
    "    logger.info('Finished build {} dataset {} at {}'.format(dataset_type, dataset_name, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the data-set builder from command line, use the following command:\n",
    "`python -m mlcore.dataset [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `[categories]`: The path to the categories file. (e.g.: *imagesets/segmentation/car_damage/categories.txt*)\n",
    "- `--annotation`: The path to the image-set annotation file, the data-set is build from. (e.g.: *imagesets/classification/car_damage/annotations.csv* for classification, *imagesets/segmentation/car_damage/via_region_data.json* for segmentation)\n",
    "- `--split`: The percentage of the data which belongs to validation set, default to *0.2* (=20%)\n",
    "- `--seed`: A random seed to reproduce splits, default to None\n",
    "- `--category-label-key`: The key, the category name can be found in the annotation file, default to *category*.\n",
    "- `--sample`: The percentage of the data which will be copied as a sample set with in a separate folder with \"_sample\" suffix. If not set, no sample data-set will be created.\n",
    "- `--type`: The type of the data-set, if not explicitly set try to infer from categories file path.\n",
    "- `--tfrecord`: Also create .tfrecord files.\n",
    "- `--join-overlapping-regions`: Whether overlapping regions of same category should be joined.\n",
    "- `--annotation-area-thresh`: Keep only annotations with minimum size (width or height) related to image size.\n",
    "- `--output`: The path of the dataset folder, default to *../datasets*.\n",
    "- `--name`: The name of the data-set, if not explicitly set try to infer from categories file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"categories\",\n",
    "                        help=\"The path to the imageset categories file.\")\n",
    "    parser.add_argument(\"--annotation\",\n",
    "                        help=\"The path to the imageset annotation file, the data-set is build from.\",\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--split\",\n",
    "                        help=\"Percentage of the data which belongs to validation set.\",\n",
    "                        type=float,\n",
    "                        default=0.2)\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        help=\"A random seed to reproduce splits.\",\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--category-label-key\",\n",
    "                        help=\"The key of the category name.\",\n",
    "                        default=CATEGORY_LABEL_KEY)\n",
    "    parser.add_argument(\"--sample\",\n",
    "                        help=\"Percentage of the data which will be copied as a sample set.\",\n",
    "                        type=float,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--type\",\n",
    "                        help=\"The type of the dataset, if not explicitly set try to infer from categories file path.\",\n",
    "                        choices=list(DatasetType),\n",
    "                        type=DatasetType,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--tfrecord\",\n",
    "                        help=\"Also create .tfrecord files.\",\n",
    "                        action=\"store_true\")\n",
    "    parser.add_argument(\"--join-overlapping-regions\",\n",
    "                        help=\"Whether overlapping regions of same category should be joined.\",\n",
    "                        action=\"store_true\")\n",
    "    parser.add_argument(\"--annotation-area-thresh\",\n",
    "                        help=\"Keep only annotations with minimum size (width or height) related to image size.\",\n",
    "                        type=float,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--output\",\n",
    "                        help=\"The path of the dataset folder.\",\n",
    "                        default=DATASET_FOLDER)\n",
    "    parser.add_argument(\"--name\",\n",
    "                        help=\"The name of the dataset, if not explicitly set try to infer from categories file path.\",\n",
    "                        default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    CATEGORY_LABEL_KEY = args.category_label_key\n",
    "\n",
    "    build_dataset(args.categories, args.output, args.annotation, args.split, args.seed, args.sample, args.type,\n",
    "                  args.tfrecord, args.join_overlapping_regions, args.annotation_area_thresh, args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
