{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from os.path import join, isdir, dirname, normpath, basename, sep, splitext, split\n",
    "from PIL import Image as PILImage\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from logging.handlers import MemoryHandler\n",
    "from mlcore.core import Type, infer_type\n",
    "from mlcore.io.core import create_folder, scan_files\n",
    "from mlcore.via.converter import region_polygon_to_rect, region_rect_to_polygon\n",
    "from mlcore.via.core import read_annotations\n",
    "from mlcore import category_tools\n",
    "import numpy as np\n",
    "import shutil\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "CATEGORY_LABEL_KEY = 'category'\n",
    "IMAGE_SET_FOLDER = 'imagesets/segmentation/car_damage'\n",
    "DEFAULT_CATEGORIES_FILE = 'categories.txt'\n",
    "DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE = 'annotations.csv'\n",
    "DEFAULT_SEGMENTATION_ANNOTATIONS_FILE = 'via_region_data.json'\n",
    "DEFAULT_SPLIT = 0.2\n",
    "DATA_SET_FOLDER = 'datasets'\n",
    "SEMANTIC_MASK_FOLDER = 'semantic_masks'\n",
    "IMAGES_TRAIN_VAL_FOLDER = 'trainval'\n",
    "IMAGES_TEST_FOLDER = 'test'\n",
    "DATA_SET_TRAIN_FOLDER = 'train'\n",
    "DATA_SET_VAL_FOLDER = 'val'\n",
    "DATA_SET_TEST_FOLDER = 'test'\n",
    "NOT_CATEGORIZED = '[NOT_CATEGORIZED]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Set\n",
    "\n",
    "> Data-Set Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data-set for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The data-set is created based on an image-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-sets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[image_set_type]*/*[image_set_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[image_set_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the image-set contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Set Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[data_set_type]*/*[data_set_name]*\n",
    "where `[data_set_type]` is the same as the corresponding `[image_set_type]` and `[data_set_name]` is the same as the corresponding `[image_set_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[data_set_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the data-set contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, name, base_path, image_set_path, categories_path, data_set_type):\n",
    "        self.name = name\n",
    "        self.base_path = base_path\n",
    "        self.image_set_path = image_set_path\n",
    "        self.categories_path = categories_path\n",
    "        self.categories = category_tools.read_categories(categories_path, data_set_type)\n",
    "        self.type = data_set_type\n",
    "        self.folder = None\n",
    "        self.train_folder = None\n",
    "        self.val_folder = None\n",
    "        self.test_folder = None\n",
    "        self.logger = self.get_logger()\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"\n",
    "        Creates the data-set folder structure, if not exist\n",
    "        \"\"\"\n",
    "\n",
    "        # create data-set folder and remove previous data if exist\n",
    "        self.folder = create_folder(join(self.base_path, str(self.type), self.name), clear=True)\n",
    "        self.logger.info(\"Created folder {}\".format(self.folder))\n",
    "        self.train_folder = create_folder(join(self.folder, DATA_SET_TRAIN_FOLDER))\n",
    "        self.logger.info(\"Created folder {}\".format(self.train_folder))\n",
    "        self.val_folder = create_folder(join(self.folder, DATA_SET_VAL_FOLDER))\n",
    "        self.logger.info(\"Created folder {}\".format(self.val_folder))\n",
    "        if isdir(join(self.image_set_path, DATA_SET_TEST_FOLDER)):\n",
    "            self.test_folder = create_folder(join(self.folder, DATA_SET_TEST_FOLDER))\n",
    "            self.logger.info(\"Created folder {}\".format(self.test_folder))\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates the images.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def copy(self, train_files, val_files, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set\n",
    "        `train_files`: The list of training images\n",
    "        `val_files`: The list of validation images\n",
    "        `test_files`: The list of test images\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def build(self, split=DEFAULT_SPLIT, seed=None, sample=None):\n",
    "        \"\"\"\n",
    "        Build the data-set. This is the main logic.\n",
    "        This method validates the images against the annotations,\n",
    "        split the image-set into train and val on given split percentage,\n",
    "        creates the data-set folders and copies the image.\n",
    "        If a sample percentage is given, a sub-set is created as sample.\n",
    "        `split`: The percentage of images which will be copied into the validation set\n",
    "        `seed`: A random seed to reproduce splits\n",
    "        `sample`: The percentage of images from train, val and test which will also from a sample set\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def get_logger(cls):\n",
    "        \"\"\"\n",
    "        Configures default logging for the system.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification data-set can be created from a classification image-set. \n",
    "All images are validated, if they belong to one of the given categories. If categories with no images are found or images belong to a category not listed in `categories.txt`, the data-set can not be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ClassificationDataSet(DataSet):\n",
    "    def __init__(self, name, base_path, image_set_path, categories_path, data_set_type, annotations_path=None):\n",
    "        super().__init__(name, base_path, image_set_path, categories_path, data_set_type)\n",
    "        self.annotations_path = annotations_path\n",
    "        self.annotations = self._read_annotations(annotations_path) if annotations_path else []\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates if all images of a classification image set belongs to a category in `categories.txt` as well as\n",
    "        that each listet category contains at least one image.\n",
    "        return: a tuple with a list of missing categories and a list of empty categories\n",
    "        \"\"\"\n",
    "\n",
    "        # validate only the trainval images, the test images have no annotations to validate\n",
    "        content_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "\n",
    "        self.logger.info('Start validate image set at {}'.format(content_folder))\n",
    "\n",
    "        base_path = join(content_folder, '')\n",
    "\n",
    "        files = list(map(lambda f: self.trim_base_path(f, base_path), scan_files(content_folder)))\n",
    "\n",
    "        self.logger.info('Found {} files at {}'.format(len(files), content_folder))\n",
    "\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'missing_categories',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just skip the category\n",
    "                    'S': 'Skip All',\n",
    "                    'a': 'Add',  # add category to category index\n",
    "                    'A': 'Add All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda tags, cats: list(filter(lambda t: t not in cats, tags)),\n",
    "                'message': '[{}] -> {} : {}Found categories not in the category index. Missing Categories: {}',\n",
    "                'transform': lambda tags, cats: list(map(lambda t: cats.append(t) if t not in cats else None, tags)),\n",
    "                'skip': lambda tags, cats: \" \".join(filter(lambda t: t not in tags, cats)),\n",
    "            },\n",
    "            {\n",
    "                'name': 'empty_categories',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # skip files with empty categories\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # keep files with empty categories\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda tags, _: not tags,\n",
    "                'message': '[{}] -> {} : {}Has no category assigned.{}',\n",
    "                'transform': lambda _, cats: cats,\n",
    "                'skip': lambda tags, cats: None,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # if no annotation file exist, generate annotations based on the folder names\n",
    "        if not self.annotations:\n",
    "            self.logger.info(\"No annotation file found, Start try to detect categories from folder structure.\")\n",
    "            self.annotations = self._annotations_from_file_paths(self.categories, files)\n",
    "            self.logger.info(\"Finished try to detect categories from folder structure.\")\n",
    "\n",
    "        handled_files = {}\n",
    "        used_categories = set([])\n",
    "\n",
    "        # skip the title column\n",
    "        for ind, (file, tags) in enumerate(self.annotations[1:]):\n",
    "            index = ind + 1  # need to count the title column\n",
    "            filename = file + \".jpg\"\n",
    "            tag_list = tags.split(\" \")\n",
    "            # validate annotation\n",
    "            for step in steps:\n",
    "                step_condition = step['condition'](tag_list, self.categories)\n",
    "                if step_condition:\n",
    "                    message = step['message'].format(index, filename, ' ', ' , '.join(step_condition))\n",
    "                    step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                    choice_op = step['choice'].lower()\n",
    "                    # if skip\n",
    "                    if choice_op == 's':\n",
    "                        handled_files[index] = step['skip'](step_condition, tag_list)\n",
    "                        message = step['message'].format(index, filename, '{} '.format(step['choices'][choice_op]),\n",
    "                                                         ' , '.join(step_condition))\n",
    "                        self.logger.info(message)\n",
    "                    else:\n",
    "                        step['transform'](step_condition, self.categories)\n",
    "                        message = step['message'].format(index, filename, '{} '.format(step['choices'][choice_op]),\n",
    "                                                         ' , '.join(step_condition))\n",
    "                        self.logger.info(message)\n",
    "\n",
    "            if tag_list:\n",
    "                used_categories.update(tag_list)\n",
    "\n",
    "            if filename not in files:\n",
    "                self.logger.info('[{}] -> {} : Annotated file do not exist, skip annotation.'.format(index, filename))\n",
    "                handled_files[index] = None\n",
    "\n",
    "        empty_categories = frozenset(self.categories) - used_categories\n",
    "        if empty_categories:\n",
    "            self.logger.info('The following categories have no images: {}'.format(\" , \".join(empty_categories)))\n",
    "\n",
    "        for index, tags in handled_files.items():\n",
    "            self.annotations[index][1] = tags\n",
    "\n",
    "        self.annotations = list(filter(lambda a: a[1] is not None, self.annotations))\n",
    "\n",
    "        self.logger.info('Finished validate image set at {}'.format(content_folder))\n",
    "\n",
    "    def copy(self, train_files, val_files, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set and generate the annotations for train and val images.\n",
    "        `train_files`: The list of training images\n",
    "        `val_files`: The list of validation images\n",
    "        `test_files`: The list of test images\n",
    "        \"\"\"\n",
    "\n",
    "        train_folder_name = basename(self.train_folder)\n",
    "        val_folder_name = basename(self.val_folder)\n",
    "\n",
    "        train_val_image_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "        test_image_folder = join(self.image_set_path, IMAGES_TEST_FOLDER)\n",
    "\n",
    "        # copy the categories files\n",
    "        shutil.copy2(self.categories_path, join(self.folder, DEFAULT_CATEGORIES_FILE))\n",
    "\n",
    "        # generate the annotations to match the processed files\n",
    "        # and add the header and append the is_valid column\n",
    "        annotations = [[h for h in (self.annotations[0] + ['is_valid'])]]\n",
    "\n",
    "        # copy train files\n",
    "        for index, filename in train_files:\n",
    "            # copy image\n",
    "            sub_folder = split(filename)[0]\n",
    "            target_folder = join(self.train_folder, sub_folder)\n",
    "            if sub_folder:\n",
    "                create_folder(target_folder)\n",
    "            shutil.copy2(join(train_val_image_folder, filename + \".jpg\"), target_folder)\n",
    "            self.logger.info('Copied {}'.format(join(self.train_folder, filename + \".jpg\")))\n",
    "            annotation = [join(train_folder_name, filename)] + self.annotations[index][1:] + [False]\n",
    "            annotations.append(annotation)\n",
    "\n",
    "        # copy val files\n",
    "        for index, filename in val_files:\n",
    "            # copy image\n",
    "            sub_folder = split(filename)[0]\n",
    "            target_folder = join(self.val_folder, sub_folder)\n",
    "            if sub_folder:\n",
    "                create_folder(target_folder)\n",
    "            shutil.copy2(join(train_val_image_folder, filename + \".jpg\"), target_folder)\n",
    "            self.logger.info('Copied {}'.format(join(self.val_folder, filename + \".jpg\")))\n",
    "            annotation = [join(val_folder_name, filename)] + self.annotations[index][1:] + [True]\n",
    "            annotations.append(annotation)\n",
    "\n",
    "        # write the modified annotation file into data-set folder\n",
    "        annotation_path = join(self.folder, DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE)\n",
    "        with open(annotation_path, mode='w') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerows(annotations)\n",
    "            self.logger.info('Wrote annotations file {}'.format(annotation_path))\n",
    "\n",
    "        # copy test_files, if exist\n",
    "        if test_files:\n",
    "            for filename in test_files:\n",
    "                shutil.copy2(join(test_image_folder, filename), self.test_folder)\n",
    "                self.logger.info('Copied {}'.format(join(self.test_folder, filename)))\n",
    "\n",
    "    def build(self, split=DEFAULT_SPLIT, seed=None, sample=None):\n",
    "        \"\"\"\n",
    "        Build the data-set. This is the main logic.\n",
    "        This method validates the images against the annotations,\n",
    "        split the image-set into train and val on given split percentage,\n",
    "        creates the data-set folders and copies the image.\n",
    "        If a sample percentage is given, a sub-set is created as sample.\n",
    "        `split`: The percentage of images which will be copied into the validation set\n",
    "        `seed`: A random seed to reproduce splits\n",
    "        `sample`: The percentage of images from train, val and test which will also from a sample set\n",
    "        \"\"\"\n",
    "        # validate the image set\n",
    "        self.validate()\n",
    "\n",
    "        # sort filenames by assigned tags\n",
    "        tags_to_files = {}\n",
    "\n",
    "        for index, (filename, tags) in enumerate(self.annotations[1:]):\n",
    "            if tags not in tags_to_files:\n",
    "                tags_to_files[tags] = []\n",
    "            tags_to_files[tags].append((index + 1, filename))\n",
    "\n",
    "        # split category files into train & val and create the sample split, if set\n",
    "        train_files = []\n",
    "        val_files = []\n",
    "        sample_train_files = []\n",
    "        sample_val_files = []\n",
    "\n",
    "        for tags, files in tags_to_files.items():\n",
    "            train, val = split_train_val_data(files, split, seed)\n",
    "            train_files.extend(train)\n",
    "            val_files.extend(val)\n",
    "\n",
    "            # if a sample data set should be created, create the splits\n",
    "            if sample:\n",
    "                _, sample_train = split_train_val_data(train, sample, seed)\n",
    "                _, sample_val = split_train_val_data(val, sample, seed)\n",
    "                sample_train_files.extend(sample_train)\n",
    "                sample_val_files.extend(sample_val)\n",
    "\n",
    "        # scan the test images if exist\n",
    "        images_test_folder = join(self.image_set_path, IMAGES_TEST_FOLDER)\n",
    "        test_files = list(map(lambda f: basename(f), scan_files(images_test_folder))) if self.test_folder else None\n",
    "        _, sample_test_files = split_train_val_data(test_files, sample, seed) if test_files and sample else (None, None)\n",
    "\n",
    "        # copy the files\n",
    "        self.copy(train_files, val_files, test_files)\n",
    "\n",
    "        if sample:\n",
    "            # create the sample data-set\n",
    "            sample_data_set = self.__class__(\"{}_sample\".format(self.name), self.base_path, self.image_set_path,\n",
    "                                             self.categories_path, self.type)\n",
    "            # assign the annotations\n",
    "            sample_data_set.annotations = self.annotations\n",
    "            # create the data set folders\n",
    "            sample_data_set.create_folders()\n",
    "            # copy the sample images\n",
    "            sample_data_set.copy(sample_train_files, sample_val_files, sample_test_files)\n",
    "\n",
    "    def _read_annotations(self, annotations_file):\n",
    "        \"\"\"\n",
    "        Reads an annotation file\n",
    "        `annotations_file`: the path to the annotation file to read\n",
    "        return: the annotations\n",
    "        \"\"\"\n",
    "        with open(annotations_file) as csv_file:\n",
    "            annotations = list(csv.reader(csv_file))\n",
    "            self.logger.info('Found {} annotations at {}'.format(len(annotations), annotations_file))\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _detect_category_index_from_paths(self, categories, file_paths):\n",
    "        for path in file_paths:\n",
    "            path_split = normpath(path).lstrip(sep).split(sep)\n",
    "            for category in categories:\n",
    "                try:\n",
    "                    index = path_split.index(category)\n",
    "                    self.logger.info(\"{} -> Found category in folder naming at index {}.\".format(path, index))\n",
    "                    return index\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            self.logger.info(\"{} -> Could not find any category in folder naming. Try next file.\".format(path))\n",
    "        return -1\n",
    "\n",
    "    def _annotations_from_file_paths(self, categories, file_paths):\n",
    "        annotations = [[\"image_name\", \"tags\"]]\n",
    "        index = self._detect_category_index_from_paths(categories, file_paths)\n",
    "        if index >= 0:\n",
    "            for path in file_paths:\n",
    "                path_split = normpath(path).lstrip(sep).split(sep)\n",
    "                annotations.append([splitext(path)[0], path_split[index]])\n",
    "        return annotations\n",
    "\n",
    "    @classmethod\n",
    "    def trim_base_path(cls, file_path, base_path):\n",
    "        if file_path.startswith(base_path):\n",
    "            file_path = file_path[len(base_path):]\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a object detection data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection data-set can be created from a segmentation or object-detection image-set.\n",
    "All images are validated against the annotations, if they contain at least one annotation and that the annotation category belongs to one of the given categories. The annotations have to be in [VIA v1](http://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.5.html) json format. Polygon annotations are converted into rectangle annotations for unique bounding-box generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ObjectDetectionDataSet(DataSet):\n",
    "    def __init__(self, name, base_path, image_set_path, categories_path, data_set_type, annotations_path=None):\n",
    "        super().__init__(name, base_path, image_set_path, categories_path, data_set_type)\n",
    "        self.annotations_path = annotations_path\n",
    "        self.annotations = read_annotations(annotations_path) if annotations_path else {}\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates, that each file has corresponding annotation with at least one region is set.\n",
    "        \"\"\"\n",
    "\n",
    "        # validate only the trainval images, the test images have no annotations to validate\n",
    "        content_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "\n",
    "        # convert the annotations before doing validation\n",
    "        self.convert_annotation()\n",
    "\n",
    "        self.logger.info('Start validate image set at {}'.format(content_folder))\n",
    "\n",
    "        files = scan_files(content_folder)\n",
    "\n",
    "        self.logger.info('Found {} files at {}'.format(len(files), content_folder))\n",
    "\n",
    "        delete_annotations = {}\n",
    "\n",
    "        for annotation_index, annotation in self.annotations.items():\n",
    "            file_name = annotation['filename']\n",
    "            regions = annotation['regions']\n",
    "\n",
    "            file_path = join(content_folder, file_name)\n",
    "\n",
    "            delete_regions = {}\n",
    "            for region_index, region in regions.items():\n",
    "                ra = region['region_attributes']\n",
    "                attribute_name = ra[CATEGORY_LABEL_KEY] if ra and CATEGORY_LABEL_KEY in ra else None\n",
    "                annotation_valid = attribute_name is not None and attribute_name in self.categories\n",
    "                if not annotation_valid:\n",
    "                    message = '{} : Region {} with category {} is not in category list, skip category.'\n",
    "                    self.logger.info(message.format(file_name, region_index, attribute_name))\n",
    "                    delete_regions[region_index] = True\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for region_index in delete_regions.keys():\n",
    "                del regions[region_index]\n",
    "\n",
    "            # validate for empty region\n",
    "            if not regions:\n",
    "                self.logger.info('{} : Has empty regions, skip file.'.format(annotation_index))\n",
    "                delete_annotations[annotation_index] = True\n",
    "\n",
    "            if file_name and file_path in files:\n",
    "                index = files.index(file_path)\n",
    "                files.pop(index)\n",
    "            else:\n",
    "                self.logger.info('{} : File of annotations do not exist, skip annotations.'.format(annotation_index))\n",
    "                delete_annotations[annotation_index] = True\n",
    "\n",
    "        for file_index, file in enumerate(files):\n",
    "            self.logger.info('[{}] -> {} : File has no annotations, skip file.'.format(file_index, file))\n",
    "\n",
    "        # delete annotations after iteration is finished\n",
    "        for annotation_index in delete_annotations.keys():\n",
    "            del self.annotations[annotation_index]\n",
    "\n",
    "        self.logger.info('Finished validate image set at {}'.format(content_folder))\n",
    "\n",
    "    def copy(self, train_files, val_files, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set, generate the annotations for train and val images.\n",
    "        `train_files`: The list of training images\n",
    "        `val_files`: The list of validation images\n",
    "        `test_files`: The list of test images\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info('Start copy files from {} to {}'.format(self.image_set_path, self.folder))\n",
    "\n",
    "        train_val_image_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "        test_image_folder = join(self.image_set_path, IMAGES_TEST_FOLDER)\n",
    "\n",
    "        # copy the categories files\n",
    "        self.logger.info('Copy file {} to {}'.format(self.categories_path, self.folder))\n",
    "        shutil.copy2(self.categories_path, join(self.folder, DEFAULT_CATEGORIES_FILE))\n",
    "\n",
    "        annotations_train = {}\n",
    "        annotations_val = {}\n",
    "\n",
    "        num_files = len(train_files)\n",
    "        self.logger.info('Start copy {} files to {}'.format(num_files, self.train_folder))\n",
    "\n",
    "        for key in train_files:\n",
    "            # copy image\n",
    "            annotation = self.annotations[key]\n",
    "            shutil.copy2(join(train_val_image_folder, annotation['filename']), self.train_folder)\n",
    "            # add annotation\n",
    "            annotations_train[key] = annotation\n",
    "        self.logger.info('Finished copy {} files to {}'.format(num_files, self.train_folder))\n",
    "\n",
    "        num_files = len(val_files)\n",
    "        self.logger.info('Start copy {} files to {}'.format(num_files, self.val_folder))\n",
    "\n",
    "        for key in val_files:\n",
    "            # copy image\n",
    "            annotation = self.annotations[key]\n",
    "            shutil.copy2(join(train_val_image_folder, annotation['filename']), self.val_folder)\n",
    "            # add annotation\n",
    "            annotations_val[key] = annotation\n",
    "\n",
    "        self.logger.info('Finished copy {} files to {}'.format(num_files, self.val_folder))\n",
    "\n",
    "        # write the split train annotations\n",
    "        if annotations_train:\n",
    "            annotations_target_path = join(self.train_folder, DEFAULT_SEGMENTATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            with open(annotations_target_path, 'w') as outfile:\n",
    "                json.dump(annotations_train, outfile)\n",
    "\n",
    "        # write the split val annotations\n",
    "        if annotations_val:\n",
    "            annotations_target_path = join(self.val_folder, DEFAULT_SEGMENTATION_ANNOTATIONS_FILE)\n",
    "            self.logger.info('Write annotations to {}'.format(annotations_target_path))\n",
    "            with open(annotations_target_path, 'w') as outfile:\n",
    "                json.dump(annotations_val, outfile)\n",
    "\n",
    "        # copy test_files, if exist\n",
    "        if test_files:\n",
    "\n",
    "            num_files = len(test_files)\n",
    "            self.logger.info('Start copy {} files to {}'.format(num_files, self.test_folder))\n",
    "\n",
    "            for filename in test_files:\n",
    "                shutil.copy2(join(test_image_folder, filename), self.test_folder)\n",
    "\n",
    "            self.logger.info('Finished copy {} files to {}'.format(num_files, self.test_folder))\n",
    "\n",
    "        self.logger.info('Finished copy files from {} to {}'.format(self.image_set_path, self.folder))\n",
    "\n",
    "    def build(self, split=DEFAULT_SPLIT, seed=None, sample=None):\n",
    "        \"\"\"\n",
    "        Build the data-set. This is the main logic.\n",
    "        This method validates the images against the annotations,\n",
    "        split the image-set into train and val on given split percentage,\n",
    "        creates the data-set folders and copies the image.\n",
    "        If a sample percentage is given, a sub-set is created as sample.\n",
    "        `split`: The percentage of images which will be copied into the validation set\n",
    "        `seed`: A random seed to reproduce splits\n",
    "        `sample`: The percentage of images from train, val and test which will also from a sample set\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info('Validation set contains {}% of the images.'.format(int(split * 100)))\n",
    "\n",
    "        # validate the image set\n",
    "        self.validate()\n",
    "\n",
    "        # split category files into train & val and create the sample split, if set\n",
    "        train_files, val_files = split_train_val_data(list(self.annotations.keys()), split, seed)\n",
    "        sample_train_files = []\n",
    "        sample_val_files = []\n",
    "\n",
    "        # if a sample data set should be created, create the splits\n",
    "        if sample:\n",
    "            _, sample_train_files = split_train_val_data(train_files, sample, seed)\n",
    "            _, sample_val_files = split_train_val_data(val_files, sample, seed)\n",
    "\n",
    "        # scan the test images if exist\n",
    "        images_test_folder = join(self.image_set_path, IMAGES_TEST_FOLDER)\n",
    "        test_files = list(map(lambda f: basename(f), scan_files(images_test_folder))) if self.test_folder else None\n",
    "        _, sample_test_files = split_train_val_data(test_files, sample, seed) if test_files and sample else (None, None)\n",
    "\n",
    "        # copy the files\n",
    "        self.copy(train_files, val_files, test_files)\n",
    "\n",
    "        if sample:\n",
    "            sample_name = \"{}_sample\".format(self.name)\n",
    "\n",
    "            self.logger.info('Start build {} data-set containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                             int(sample * 100),\n",
    "                                                                                             self.base_path))\n",
    "\n",
    "            # create the sample data-set\n",
    "            sample_data_set = self.__class__(sample_name, self.base_path, self.image_set_path, self.categories_path,\n",
    "                                             self.type)\n",
    "            # assign the converted annotations\n",
    "            sample_data_set.annotations = self.annotations\n",
    "\n",
    "            # create the data set folders\n",
    "            sample_data_set.create_folders()\n",
    "            # copy the sample images\n",
    "            sample_data_set.copy(sample_train_files, sample_val_files, sample_test_files)\n",
    "\n",
    "            self.logger.info('Finished build {} data-set containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                                int(sample * 100),\n",
    "                                                                                                self.base_path))\n",
    "\n",
    "    def convert_annotation(self):\n",
    "        \"\"\"\n",
    "        Converts segmentation regions from polygon to rectangle, if exist\n",
    "        \"\"\"\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        content_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'position',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    't': 'Trim',  # transform the annotation\n",
    "                    'T': 'Trim All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: p_min < 0 or p_max >= size,\n",
    "                'message': '{} -> {} : {}Exceeds image {}. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: max(min(p, size - 1), 0),\n",
    "            },\n",
    "            {\n",
    "                'name': 'size',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, _: p_max - p_min <= 1,\n",
    "                'message': '{} -> {} : {}Shape {} is <= 1 pixel. \\n Box \\n x: {} \\n y: {} \\n x_max: {} \\n y_max: {}',\n",
    "                'transform': lambda p, size=0: p,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.logger.info('Start convert image annotations from {}'.format(self.annotations_path))\n",
    "\n",
    "        for annotation_index, annotation in self.annotations.items():\n",
    "            file_name = annotation['filename']\n",
    "            regions = annotation['regions']\n",
    "\n",
    "            if not regions:\n",
    "                continue\n",
    "\n",
    "            with PILImage.open(join(content_folder, file_name)) as image:\n",
    "                image_width, image_height = image.size\n",
    "\n",
    "                delete_regions = {}\n",
    "                for region_index, region in regions.items():\n",
    "                    # convert from polygon to rect if needed\n",
    "                    region = region_polygon_to_rect(region)\n",
    "                    shape_attributes = region['shape_attributes']\n",
    "\n",
    "                    for step in steps:\n",
    "                        # validate the shape size\n",
    "                        x_min = shape_attributes['x']\n",
    "                        x_max = shape_attributes['x'] + shape_attributes['width']\n",
    "                        y_min = shape_attributes['y']\n",
    "                        y_max = shape_attributes['y'] + shape_attributes['height']\n",
    "\n",
    "                        width_condition = step['condition'](x_min, x_max, image_width)\n",
    "                        height_condition = step['condition'](y_min, y_max, image_height)\n",
    "                        if width_condition or height_condition:\n",
    "                            size_message = ['width'] if width_condition else []\n",
    "                            size_message.extend(['height'] if height_condition else [])\n",
    "                            message = step['message'].format(annotation_index, region_index, ' ',\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             x_min, y_min, x_max, y_max)\n",
    "\n",
    "                            step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                            choice_op = step['choice'].lower()\n",
    "                            # if skip the shapes\n",
    "                            if choice_op == 's':\n",
    "                                delete_regions[region_index] = True\n",
    "                                message = step['message'].format(annotation_index, region_index,\n",
    "                                                                 '{} '.format(step['choices'][choice_op]),\n",
    "                                                                 ' and '.join(size_message),\n",
    "                                                                 x_min, y_min, x_max, y_max)\n",
    "                                self.logger.info(message)\n",
    "\n",
    "                                break\n",
    "                            else:\n",
    "                                x_min, x_max = tuple(map(partial(step['transform'], size=image_width), [x_min, x_max]))\n",
    "                                y_min, y_max = tuple(map(partial(step['transform'], size=image_height), [y_min, y_max]))\n",
    "                                shape_attributes['x'] = x_min\n",
    "                                shape_attributes['width'] = x_max - x_min\n",
    "                                shape_attributes['y'] = y_min\n",
    "                                shape_attributes['height'] = y_max - y_min\n",
    "\n",
    "                                message = step['message'].format(annotation_index, region_index,\n",
    "                                                                 '{} '.format(step['choices'][choice_op]),\n",
    "                                                                 ' and '.join(size_message),\n",
    "                                                                 x_min, y_min, x_max, y_max)\n",
    "                                self.logger.info(message)\n",
    "\n",
    "                # delete regions after iteration is finished\n",
    "                for region_index in delete_regions.keys():\n",
    "                    del regions[region_index]\n",
    "\n",
    "        print('Finished convert image annotations from {}'.format(self.annotations_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a segmentation data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation data-set can be created from a segmentation image-set.\n",
    "All images are validated against the annotations, if they contain at least one annotation and that the annotation category belongs to one of the given categories. The annotations have to be in [VIA v1](http://www.robots.ox.ac.uk/~vgg/software/via/via-1.0.5.html) json format. Rectangle annotations are converted into polygon annotations for unique segment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class SegmentationDataSet(ObjectDetectionDataSet):\n",
    "    def __init__(self, name, base_path, image_set_path, categories_path, data_set_type, annotations_path=None):\n",
    "        super().__init__(name, base_path, image_set_path, categories_path, data_set_type, annotations_path)\n",
    "        self.semantic_mask_folder = None\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"\n",
    "        Creates the data-set folder structure, if not exist\n",
    "        \"\"\"\n",
    "        super().create_folders()\n",
    "\n",
    "        # create semantic mask file folder and remove previous data if exist\n",
    "        self.semantic_mask_folder = create_folder(join(self.folder, SEMANTIC_MASK_FOLDER), clear=True)\n",
    "        self.logger.info(\"Created folder {}\".format(self.semantic_mask_folder))\n",
    "\n",
    "    def copy(self, train_files, val_files, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the data-set, generate the annotations for train and val images\n",
    "        and generate the semantic masks.\n",
    "        `train_files`: The list of training images\n",
    "        `val_files`: The list of validation images\n",
    "        `test_files`: The list of test images\n",
    "        \"\"\"\n",
    "        super().copy(train_files, val_files, test_files)\n",
    "\n",
    "        # save semantic annotations\n",
    "        self._save_semantic_masks(train_files + val_files)\n",
    "\n",
    "    def convert_annotation(self):\n",
    "        \"\"\"\n",
    "        Converts segmentation regions from rectangle to polygon, if exist\n",
    "        \"\"\"\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        content_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "\n",
    "        steps = [\n",
    "            {\n",
    "                'name': 'position',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    't': 'Trim',  # transform the annotation\n",
    "                    'T': 'Trim All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, size: p_min < 0 or p_max >= size,\n",
    "                'message': '{} -> {} : {}Exceeds image {}. \\n Points \\n x: {} \\n y: {}',\n",
    "                'transform': lambda p, size=0: max(min(p, size - 1), 0),\n",
    "            },\n",
    "            {\n",
    "                'name': 'size',\n",
    "                'choices': {\n",
    "                    's': 'Skip',  # just delete the annotation\n",
    "                    'S': 'Skip All',\n",
    "                    'k': 'Keep',  # transform the annotation (in this case do nothing)\n",
    "                    'K': 'Keep All',\n",
    "                },\n",
    "                'choice': None,\n",
    "                'condition': lambda p_min, p_max, _: p_max - p_min <= 1,\n",
    "                'message': '{} -> {} : {}Shape {} is <= 1 pixel. \\n Points \\n x: {} \\n y: {}',\n",
    "                'transform': lambda p, size=0: p,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.logger.info('Start convert image annotations from {}'.format(self.annotations_path))\n",
    "\n",
    "        for annotation_index, annotation in self.annotations.items():\n",
    "            file_name = annotation['filename']\n",
    "            regions = annotation['regions']\n",
    "\n",
    "            if not regions:\n",
    "                continue\n",
    "\n",
    "            with PILImage.open(join(content_folder, file_name)) as image:\n",
    "                image_width, image_height = image.size\n",
    "\n",
    "                delete_regions = {}\n",
    "                for region_index, region in regions.items():\n",
    "                    # convert from rect to polygon if needed\n",
    "                    region = region_rect_to_polygon(region)\n",
    "                    shape_attributes = region['shape_attributes']\n",
    "\n",
    "                    for step in steps:\n",
    "                        # validate the shape size\n",
    "                        x_min = min(shape_attributes['all_points_x'])\n",
    "                        x_max = max(shape_attributes['all_points_x'])\n",
    "                        y_min = min(shape_attributes['all_points_y'])\n",
    "                        y_max = max(shape_attributes['all_points_y'])\n",
    "\n",
    "                        width_condition = step['condition'](x_min, x_max, image_width)\n",
    "                        height_condition = step['condition'](y_min, y_max, image_height)\n",
    "                        if width_condition or height_condition:\n",
    "                            size_message = ['width'] if width_condition else []\n",
    "                            size_message.extend(['height'] if height_condition else [])\n",
    "                            message = step['message'].format(annotation_index, region_index, ' ',\n",
    "                                                             ' and '.join(size_message),\n",
    "                                                             shape_attributes['all_points_x'],\n",
    "                                                             shape_attributes['all_points_y'])\n",
    "\n",
    "                            step['choice'] = input_feedback(message, step['choice'], step['choices'])\n",
    "\n",
    "                            choice_op = step['choice'].lower()\n",
    "                            # if skip the shapes\n",
    "                            if choice_op == 's':\n",
    "                                delete_regions[region_index] = True\n",
    "                                message = step['message'].format(annotation_index, region_index,\n",
    "                                                                 '{} '.format(step['choices'][choice_op]),\n",
    "                                                                 ' and '.join(size_message),\n",
    "                                                                 shape_attributes['all_points_x'],\n",
    "                                                                 shape_attributes['all_points_y'])\n",
    "                                self.logger.info(message)\n",
    "\n",
    "                                break\n",
    "                            else:\n",
    "                                shape_attributes['all_points_x'] = list(map(partial(step['transform'],\n",
    "                                                                                    size=image_width),\n",
    "                                                                            shape_attributes['all_points_x']))\n",
    "                                shape_attributes['all_points_y'] = list(map(partial(step['transform'],\n",
    "                                                                                    size=image_height),\n",
    "                                                                            shape_attributes['all_points_y']))\n",
    "\n",
    "                                message = step['message'].format(annotation_index, region_index,\n",
    "                                                                 '{} '.format(step['choices'][choice_op]),\n",
    "                                                                 ' and '.join(size_message),\n",
    "                                                                 shape_attributes['all_points_x'],\n",
    "                                                                 shape_attributes['all_points_y'])\n",
    "                                self.logger.info(message)\n",
    "\n",
    "                # delete regions after iteration is finished\n",
    "                for region_index in delete_regions.keys():\n",
    "                    del regions[region_index]\n",
    "\n",
    "        print('Finished convert image annotations from {}'.format(self.annotations_path))\n",
    "\n",
    "    def _save_semantic_masks(self, keys):\n",
    "        \"\"\"\n",
    "        Create semantic segmentation mask png files out of the annotations.\n",
    "        The mask file name is the same as the image file name but is stored in png format.\n",
    "        `keys`: The annotation keys to create the segmentation masks for\n",
    "        \"\"\"\n",
    "        from skimage import draw\n",
    "\n",
    "        num_masks = len(keys)\n",
    "        self.logger.info('Start create {} segmentation masks in {}'.format(num_masks, self.semantic_mask_folder))\n",
    "\n",
    "        # only the trainval images have annotation, not the test images\n",
    "        content_folder = join(self.image_set_path, IMAGES_TRAIN_VAL_FOLDER)\n",
    "\n",
    "        for index, key in enumerate(keys):\n",
    "            annotation = self.annotations[key]\n",
    "            file_name = annotation['filename']\n",
    "            regions = annotation['regions']\n",
    "\n",
    "            if not regions:\n",
    "                continue\n",
    "\n",
    "            with PILImage.open(join(content_folder, file_name)) as image:\n",
    "                image_width, image_height = image.size\n",
    "\n",
    "                # Convert polygons to a bitmap mask of shape\n",
    "                # [height, width]\n",
    "                mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "\n",
    "                # sort the regions by category priority for handling pixels which are assigned to more than one category\n",
    "                # the category with higher index overpaint the category with lower index\n",
    "                for region in sorted(regions.values(),\n",
    "                                     key=lambda r: self.categories.index(r['region_attributes'][CATEGORY_LABEL_KEY])):\n",
    "                    shape_attributes = region['shape_attributes']\n",
    "                    region_attributes = region['region_attributes']\n",
    "                    category = region_attributes[CATEGORY_LABEL_KEY]\n",
    "                    class_id = self.categories.index(category) + 1\n",
    "\n",
    "                    # Get indexes of pixels inside the polygon and set them to 1\n",
    "                    rr, cc = draw.polygon(shape_attributes['all_points_y'], shape_attributes['all_points_x'])\n",
    "                    mask[rr, cc] = class_id\n",
    "\n",
    "                # save the semantic mask\n",
    "                im = PILImage.fromarray(mask)\n",
    "                mask_path = join(self.semantic_mask_folder, splitext(file_name)[0] + '.png')\n",
    "                im.save(mask_path)\n",
    "\n",
    "                self.logger.info('{} / {} - Created segmentation mask {}'.format(index + 1, num_masks, mask_path))\n",
    "\n",
    "        self.logger.info('Finish create {} segmentation masks in {}'.format(num_masks, self.semantic_mask_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def split_train_val_data(data, val_size=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits the images in train and validation set\n",
    "    `data`: the data to split\n",
    "    `val_size`: the size of the validation set in percentage\n",
    "    `seed`: A random seed to reproduce splits.\n",
    "    return: the splittet train, validation images\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test = train_test_split(data, random_state=seed, test_size=val_size) if len(data) > 1 else (data, [])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def input_feedback(msg, choice, choices):\n",
    "    # if decision is already made for all contents, skip feedback\n",
    "    if not (choice and choice.isupper()):\n",
    "        prompt = '{} \\n choices: {} '.format(msg, ', '.join(['{} ({})'.format(k, v) for k, v in choices.items()]))\n",
    "        while True:\n",
    "            choice = input(prompt)\n",
    "            if choice in choices:\n",
    "                break\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "    \"\"\"\n",
    "    log_memory_handler = MemoryHandler(1, flushLevel=logging_level)\n",
    "    log_memory_handler.setLevel(logging_level)\n",
    "\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setLevel(logging_level)\n",
    "\n",
    "    logger.addHandler(log_memory_handler)\n",
    "    logger.addHandler(stdout_handler)\n",
    "\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    return log_memory_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data-set from an image-set. Handles currently classification and segmentation image-sets taken from the image-set-type, which is the parent folder, the image-set folder is located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def build_data_set(category_file_path, annotation_file_path, split, seed, sample, data_set_type, output, data_set_name):\n",
    "    \"\"\"\n",
    "    Build the data-set for training, Validation and test\n",
    "    `category_file_path`: the filename of the categories file\n",
    "    `annotation_file_path`: the file path to the annotation file\n",
    "    `split`: the size of the validation set as percentage\n",
    "    `seed`: random seed to reproduce splits\n",
    "    `sample`: the size of the sample set as percentage\n",
    "    `data_set_type`: the type of the data-set, if not set infer from the category file path\n",
    "    `output`: the dataset base folder to build the dataset in\n",
    "    `data_set_name`: the name of the data-set, if not set infer from the category file path\n",
    "    \"\"\"\n",
    "    log_memory_handler = configure_logging()\n",
    "\n",
    "    # try to infer the data-set type if not explicitly set\n",
    "    if data_set_type is None:\n",
    "        try:\n",
    "            data_set_type = infer_type(category_file_path)\n",
    "        except ValueError as e:\n",
    "            logger.error(e)\n",
    "            return\n",
    "\n",
    "    path = dirname(category_file_path)\n",
    "\n",
    "    # try to infer the data-set name if not explicitly set\n",
    "    if data_set_name is None:\n",
    "        data_set_name = basename(path)\n",
    "\n",
    "    logger.info('Build parameters:')\n",
    "    logger.info(' '.join(sys.argv[1:]))\n",
    "    logger.info('Build configuration:')\n",
    "    logger.info('category_file_path: {}'.format(category_file_path))\n",
    "    logger.info('annotation_file_path: {}'.format(annotation_file_path))\n",
    "    logger.info('split: {}'.format(split))\n",
    "    logger.info('seed: {}'.format(seed))\n",
    "    logger.info('sample: {}'.format(sample))\n",
    "    logger.info('type: {}'.format(data_set_type))\n",
    "    logger.info('output: {}'.format(output))\n",
    "    logger.info('name: {}'.format(data_set_name))\n",
    "\n",
    "    data_set = None\n",
    "    logger.info('Start build {} data-set {} at {}'.format(data_set_type, data_set_name, output))\n",
    "\n",
    "    if data_set_type == Type.IMAGE_CLASSIFICATION:\n",
    "        data_set = ClassificationDataSet(data_set_name, output, path, category_file_path, data_set_type,\n",
    "                                         annotation_file_path)\n",
    "    elif data_set_type == Type.IMAGE_SEGMENTATION:\n",
    "        data_set = SegmentationDataSet(data_set_name, output, path, category_file_path, data_set_type,\n",
    "                                       annotation_file_path)\n",
    "    elif data_set_type == Type.IMAGE_OBJECT_DETECTION:\n",
    "        data_set = ObjectDetectionDataSet(data_set_name, output, path, category_file_path, data_set_type,\n",
    "                                          annotation_file_path)\n",
    "\n",
    "    if data_set:\n",
    "        # create the data set folders\n",
    "        logger.info(\"Start create the data-set folders at {}\".format(data_set.base_path))\n",
    "        data_set.create_folders()\n",
    "        logger.info(\"Finished create the data-set folders at {}\".format(data_set.base_path))\n",
    "\n",
    "        # create the build log file\n",
    "        log_file_name = datetime.now().strftime(\"build_%Y.%m.%d-%H.%M.%S.log\")\n",
    "        file_handler = logging.FileHandler(join(data_set.folder, log_file_name), encoding=\"utf-8\")\n",
    "        log_memory_handler.setTarget(file_handler)\n",
    "\n",
    "        # build the data set\n",
    "        data_set.build(split, seed, sample)\n",
    "\n",
    "    logger.info('Finished build {} data-set {} at {}'.format(data_set_type, data_set_name, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the data-set builder from command line, use the following command:\n",
    "`python -m mlcore.dataset [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `[categories]`: The path to the categories file. (e.g.: *categories.txt*)\n",
    "- `--annotation`: The path to the image-set annotation file, the data-set is build from. (e.g.: *imagesets/classification/car_damage/annotations.csv* for classification, *imagesets/segmentation/car_damage/via_region_data.json* for segmentation)\n",
    "- `--split`: The percentage of the data which belongs to validation set, default to *0.2* (=20%)\n",
    "- `--seed`: A random seed to reproduce splits, default to None\n",
    "- `--category-label-key`: The key, the category name can be found in the annotation file, default to *category*.\n",
    "- `--sample`: The percentage of the data which will be copied as a sample set with in a separate folder with \"_sample\" suffix. If not set, no sample data-set will be created.\n",
    "- `--type`: The type of the data-set, if not explicitly set try to infer from categories file path.\n",
    "- `--output`: The path of the dataset folder, default to *../datasets*.\n",
    "- `--name`: The name of the data-set, if not explicitly set try to infer from categories file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"categories\",\n",
    "                        help=\"The path to the image-set categories file.\")\n",
    "    parser.add_argument(\"--annotation\",\n",
    "                        help=\"The path to the image-set annotation file, the data-set is build from.\",\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--split\",\n",
    "                        help=\"Percentage of the data which belongs to validation set.\",\n",
    "                        type=float,\n",
    "                        default=0.2)\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        help=\"A random seed to reproduce splits.\",\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--category-label-key\",\n",
    "                        help=\"The key of the category name.\",\n",
    "                        default=CATEGORY_LABEL_KEY)\n",
    "    parser.add_argument(\"--sample\",\n",
    "                        help=\"Percentage of the data which will be copied as a sample set.\",\n",
    "                        type=float,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--type\",\n",
    "                        help=\"The type of the data-set, if not explicitly set try to infer from categories file path.\",\n",
    "                        choices=list(Type),\n",
    "                        type=Type,\n",
    "                        default=None)\n",
    "    parser.add_argument(\"--output\",\n",
    "                        help=\"The path of the data-set folder.\",\n",
    "                        default=DATA_SET_FOLDER)\n",
    "    parser.add_argument(\"--name\",\n",
    "                        help=\"The name of the data-set, if not explicitly set try to infer from categories file path.\",\n",
    "                        default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    CATEGORY_LABEL_KEY = args.category_label_key\n",
    "\n",
    "    build_data_set(args.categories, args.annotation, args.split, args.seed, args.sample, args.type, args.output,\n",
    "                   args.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
