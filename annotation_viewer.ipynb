{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp annotation_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "import aiforce.image.opencv_tools as opencv_tools\n",
    "import aiforce.image.pillow_tools as pillow_tools\n",
    "from enum import Enum\n",
    "from aiforce import annotation as annotation_package\n",
    "from aiforce.core import list_subclasses, parse_known_args_with_help\n",
    "from aiforce.annotation.core import AnnotationAdapter, SubsetType\n",
    "from aiforce.annotation.core import RegionShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "WINDOW_NAME = 'Annotation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Viewer\n",
    "> Simple Annotation Viewer using OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/annotation_viewer.png\" alt=\"AnnotationViewer\" width=\"800\" caption=\"The Annotation Viewer.\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current supported annotations:\n",
    "- circle\n",
    "- ellipse\n",
    "- point\n",
    "- polyline\n",
    "- rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported keyboard commands:\n",
    "- **n**: Go to next annotation\n",
    "- **b**: Go to previous annotation\n",
    "- **q**: Quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageLoader(Enum):\n",
    "    \"\"\"\n",
    "    Currently supported image loader libraries.\n",
    "    \"\"\"\n",
    "    OPEN_CV = 'open_cv'\n",
    "    PILLOW = 'pillow'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def show_annotated_images(annotation_adapter, image_loader, max_width=0, max_height=0):\n",
    "    \"\"\"\n",
    "    Show images with corresponding annotations.\n",
    "    Images are shown one at a time with switching by using the arrow left/right keys.\n",
    "    `annotation_adapter`: The annotation adapter to use\n",
    "    `image_loader`: The aimage loader library to use\n",
    "    `max_width`: The maximum width to scale the image for visibility.\n",
    "    `max_height`: The maximum height to scale the image for visibility.\n",
    "    \"\"\"\n",
    "    categories = annotation_adapter.read_categories()\n",
    "    annotations = annotation_adapter.read_annotations(SubsetType.TRAINVAL)\n",
    "    len_annotations = len(annotations)\n",
    "\n",
    "    if len_annotations == 0:\n",
    "        logging.error(\"No Annotations found\")\n",
    "        return\n",
    "\n",
    "    index = 0\n",
    "    annotation_keys = list(annotations.keys())\n",
    "\n",
    "    logging.info(\"Keys to use:\")\n",
    "    logging.info(\"n = Next Image\")\n",
    "    logging.info(\"b = Previous Image\")\n",
    "    logging.info(\"q = Quit\")\n",
    "\n",
    "    logging.info(\"Annotations to view: {}\".format(len_annotations))\n",
    "\n",
    "    while True:\n",
    "        annotation_id = annotation_keys[index]\n",
    "        annotation = annotations[annotation_id]\n",
    "        logging.info(\"View Image {}/{}: {}\".format(index + 1, len_annotations, annotation.file_path))\n",
    "        if image_loader == ImageLoader.PILLOW:\n",
    "            img, _, __ = pillow_tools.get_image_size(annotation.file_path)\n",
    "            img = opencv_tools.from_pillow_image(img)\n",
    "        elif image_loader == ImageLoader.OPEN_CV:\n",
    "            img, _, __ = opencv_tools.get_image_size(annotation.file_path)\n",
    "        else:\n",
    "            logging.error(\"Unsupported image loader\")\n",
    "            img = None\n",
    "\n",
    "        if img is None:\n",
    "            logging.info(\"Image not found at {}\".format(annotation.file_path))\n",
    "            img = np.zeros(shape=(1, 1, 3))\n",
    "\n",
    "        if annotation.regions:\n",
    "            logging.info(\"Found {} regions\".format(len(annotation.regions)))\n",
    "            for region_index, region in enumerate(annotation.regions):\n",
    "                points = list(zip(region.points_x, region.points_y))\n",
    "                logging.info(\"Found {} of category {} with {} points: {}\".format(region.shape,\n",
    "                                                                                 ','.join(region.labels),\n",
    "                                                                                 len(points), points))\n",
    "                if region.shape == RegionShape.CIRCLE:\n",
    "                    img = cv2.circle(img, points[0], region.radius_x, (0, 255, 255), 2)\n",
    "                elif region.shape == RegionShape.ELLIPSE:\n",
    "                    img = cv2.ellipse(img, points[0], (region.radius_x, region.radius_y), 0, 0, 360,\n",
    "                                      (0, 255, 255), 2)\n",
    "                elif region.shape == RegionShape.POINT:\n",
    "                    img = cv2.circle(img, points[0], 1, (0, 255, 255), 2)\n",
    "                elif region.shape == RegionShape.POLYGON:\n",
    "                    pts = np.array(points, np.int32)\n",
    "                    pts = pts.reshape((-1, 1, 2))\n",
    "                    img = cv2.polylines(img, [pts], True, (0, 255, 255), 2)\n",
    "                elif region.shape == RegionShape.RECTANGLE:\n",
    "                    img = cv2.rectangle(img, points[0], points[1], (0, 255, 255), 2)\n",
    "\n",
    "        if max_width and max_height:\n",
    "            img = opencv_tools.fit_to_max_size(img, max_width, max_height)\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, img)\n",
    "        cv2.setWindowTitle(WINDOW_NAME, \"Image {}/{}\".format(index + 1, len_annotations))\n",
    "\n",
    "        k = cv2.waitKey(0)\n",
    "        if k == ord('q'):    # 'q' key to stop\n",
    "            break\n",
    "        elif k == ord('b'):\n",
    "            index = max(0, index - 1)\n",
    "        elif k == ord('n'):\n",
    "            index = min(len_annotations - 1, index + 1)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "\n",
    "    :param logging_level: The logging level to use.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the annotation viewer from command line, use the following command:\n",
    "`python -m mlcore.via.annotation_viewer [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `-a`, `--annotation`: The annotation adapter to read the annotations (e.g.: *VIAAnnotationAdapter*)\n",
    "- `--max-width`: The maximum width to scale the image for visibility, default is no scale\n",
    "- `--max-height`: The maximum height to scale the image for visibility, default is no scale\n",
    "- `--image_loader`: The image library for reading the image, default is `Pillow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    configure_logging()\n",
    "\n",
    "    # read annotation adapters to use\n",
    "    adapters = list_subclasses(annotation_package, AnnotationAdapter)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-a\",\n",
    "                        \"--annotation\",\n",
    "                        help=\"The annotation adapter to read the annotations.\",\n",
    "                        type=str,\n",
    "                        choices=adapters.keys(),\n",
    "                        required=True)\n",
    "    parser.add_argument(\"--max-width\",\n",
    "                        help=\"The maximum width to scale the image for visibility.\",\n",
    "                        type=int,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--max-height\",\n",
    "                        help=\"The maximum height to scale the image for visibility.\",\n",
    "                        type=int,\n",
    "                        default=0)\n",
    "    parser.add_argument(\"--image_loader\",\n",
    "                        help=\"The image library for reading the image.\",\n",
    "                        choices=list(ImageLoader),\n",
    "                        type=ImageLoader,\n",
    "                        default=ImageLoader.PILLOW)\n",
    "\n",
    "    argv = sys.argv\n",
    "    args, argv = parse_known_args_with_help(parser, argv)\n",
    "\n",
    "    adapter_class = adapters[args.annotation]\n",
    "\n",
    "    # parse the annotation arguments\n",
    "    annotation_parser = getattr(adapter_class, 'argparse')()\n",
    "    annotation_args, argv = parse_known_args_with_help(annotation_parser, argv)\n",
    "\n",
    "    show_annotated_images(adapter_class(**vars(annotation_args)), args.image_loader, args.max_width, args.max_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
