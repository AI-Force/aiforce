{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import argparse\n",
    "import logging\n",
    "from abc import ABC\n",
    "from os.path import join, basename, dirname\n",
    "from aiforce.core import assign_arg_prefix\n",
    "from aiforce.annotation.core import AnnotationAdapter, SubsetType\n",
    "from aiforce.image.pillow_tools import assign_exif_orientation, write_exif_metadata\n",
    "from aiforce.io.core import create_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dataset Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The dataset is created based on an imageset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imageset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagesets are collected images to build a dataset from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[imageset_type]*/*[imageset_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[imageset_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the imageset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-sets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[dataset_type]*/*[dataset_name]*\n",
    "where `[dataset_type]` is the same as the corresponding `[imageset_type]` and `[dataset_name]` is the same as the corresponding `[imageset_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[dataset_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the dataset contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    Dataset base class to build datasets.\n",
    "    `args`: the arguments containing the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_SPLIT = 0.2\n",
    "\n",
    "    def __init__(self, input_adapter: AnnotationAdapter, output_adapter: AnnotationAdapter, split=None, seed=None,\n",
    "                 sample=None):\n",
    "        self.input_adapter = input_adapter\n",
    "        self.output_adapter = output_adapter\n",
    "        self.split = self.DEFAULT_SPLIT if split is None else split\n",
    "        self.seed = seed\n",
    "        self.sample = sample\n",
    "        self.categories = input_adapter.read_categories()\n",
    "        self.annotations = input_adapter.read_annotations()\n",
    "\n",
    "    @classmethod\n",
    "    def argparse(cls, prefix=None):\n",
    "        \"\"\"\n",
    "        Returns the argument parser containing argument definition for command line use.\n",
    "        `prefix`: a parameter prefix to set, if needed\n",
    "        return: the argument parser\n",
    "        \"\"\"\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(assign_arg_prefix('--split', prefix),\n",
    "                            dest=\"split\",\n",
    "                            help=\"Percentage of the data which belongs to validation set.\",\n",
    "                            type=float,\n",
    "                            default=0.2)\n",
    "        parser.add_argument(assign_arg_prefix('--seed', prefix),\n",
    "                            dest=\"seed\",\n",
    "                            help=\"A random seed to reproduce splits.\",\n",
    "                            type=int,\n",
    "                            default=None)\n",
    "        parser.add_argument(assign_arg_prefix('--sample', prefix),\n",
    "                            dest=\"sample\",\n",
    "                            help=\"Percentage of the data which will be copied as a sample set.\",\n",
    "                            type=float,\n",
    "                            default=0)\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def create_folders(self):\n",
    "        \"\"\"\n",
    "        Creates the dataset folder structure, if not exist\n",
    "        \"\"\"\n",
    "        output_folder = create_folder(self.output_adapter.path, clear=True)\n",
    "        logger.info(\"Created folder {}\".format(output_folder))\n",
    "\n",
    "    def build_info(self):\n",
    "        \"\"\"\n",
    "        Log build information\n",
    "        \"\"\"\n",
    "        logger.info('Build configuration:')\n",
    "        logger.info('input_adapter: {}'.format(type(self.input_adapter).__name__))\n",
    "        logger.info('input_path: {}'.format(self.input_adapter.path))\n",
    "        logger.info('output_adapter: {}'.format(type(self.output_adapter).__name__))\n",
    "        logger.info('output_path: {}'.format(self.output_adapter.path))\n",
    "        logger.info('split: {}'.format(self.split))\n",
    "        logger.info('seed: {}'.format(self.seed))\n",
    "        logger.info('sample: {}'.format(self.sample))\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates the annotations.\n",
    "        return: The skipped annotations\n",
    "        \"\"\"\n",
    "        # validate only the trainval images, the test images have no annotations to validate\n",
    "        logger.info('Start validate data at {}'.format(self.input_adapter.path))\n",
    "\n",
    "        files = self.input_adapter.list_files()\n",
    "\n",
    "        logger.info('Found {} files at {}'.format(len(files), self.input_adapter.path))\n",
    "\n",
    "        delete_annotations = {}\n",
    "        used_categories = set([])\n",
    "\n",
    "        for annotation_id, annotation in self.annotations.items():\n",
    "\n",
    "            delete_regions = {}\n",
    "            for index, region in enumerate(annotation.regions):\n",
    "                len_labels = len(region.labels)\n",
    "                region_valid = len_labels and len(set(region.labels) & set(self.categories)) == len_labels\n",
    "                if not region_valid:\n",
    "                    message = '{} : Region {} with category {} is not in category list, skip region.'\n",
    "                    logger.info(message.format(annotation.file_path, index, ','.join(region.labels)))\n",
    "\n",
    "                    delete_regions[index] = True\n",
    "                else:\n",
    "                    # update the used regions\n",
    "                    used_categories.update(region.labels)\n",
    "\n",
    "            # delete regions after iteration is finished\n",
    "            for index in sorted(list(delete_regions.keys()), reverse=True):\n",
    "                del annotation.regions[index]\n",
    "\n",
    "            # validate for empty region\n",
    "            if not annotation.regions:\n",
    "                logger.info('{} : Has empty regions, skip annotation.'.format(annotation.file_path))\n",
    "                delete_annotations[annotation_id] = True\n",
    "            # validate for file exist\n",
    "            elif annotation.file_path not in files:\n",
    "                logger.info('{} : File of annotations do not exist, skip annotations.'.format(annotation.file_path))\n",
    "                delete_annotations[annotation_id] = True\n",
    "            else:\n",
    "                files.pop(files.index(annotation.file_path))\n",
    "\n",
    "        for index, file in enumerate(files):\n",
    "            logger.info('[{}] -> {} : File has no annotations, skip file.'.format(index, file))\n",
    "\n",
    "        # list unused categories\n",
    "        empty_categories = frozenset(self.categories) - used_categories\n",
    "        if empty_categories:\n",
    "            logger.info('The following categories have no images: {}'.format(\" , \".join(empty_categories)))\n",
    "\n",
    "        # delete annotations after iteration is finished\n",
    "        for index in delete_annotations.keys():\n",
    "            del self.annotations[index]\n",
    "\n",
    "        logger.info('Finished validate image set at {}'.format(self.input_adapter.path))\n",
    "        return delete_annotations\n",
    "\n",
    "    def copy(self, train_annotation_keys, val_annotation_keys, test_files=None):\n",
    "        \"\"\"\n",
    "        Copy the images to the dataset.\n",
    "        `train_annotation_keys`: The list of training annotation keys\n",
    "        `val_annotation_keys`: The list of validation annotation keys\n",
    "        `test_files`: The list of test file paths\n",
    "        return: A tuple containing train, val and test target file paths\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info('Start copy annotations from {} to {}'.format(self.input_adapter.path,\n",
    "                                                                  self.output_adapter.path))\n",
    "\n",
    "        # copy the categories files\n",
    "        logger.info('Write categories to {}'.format(self.output_adapter.path))\n",
    "        self.output_adapter.write_categories(self.categories)\n",
    "\n",
    "        logger.info('Write {} annotations to {}'.format(str(SubsetType.TRAIN), self.output_adapter.path))\n",
    "        annotations_train = dict(zip(train_annotation_keys, [self.annotations[key] for key in train_annotation_keys]))\n",
    "        train_targets = self.output_adapter.write_annotations(annotations_train, SubsetType.TRAIN)\n",
    "        logger.info('Write {} annotations to {}'.format(str(SubsetType.VAL), self.output_adapter.path))\n",
    "        annotations_val = dict(zip(val_annotation_keys, [self.annotations[key] for key in val_annotation_keys]))\n",
    "        val_targets = self.output_adapter.write_annotations(annotations_val, SubsetType.VAL)\n",
    "        logger.info('Write {} files to {}'.format(str(SubsetType.TEST), self.output_adapter.path))\n",
    "        test_targets = self.output_adapter.write_files(test_files, SubsetType.TEST) if test_files else []\n",
    "\n",
    "        return train_targets, val_targets, test_targets\n",
    "\n",
    "    def build(self, validate=True):\n",
    "        \"\"\"\n",
    "        Build the data-set. This is the main logic.\n",
    "        This method validates the images against the annotations,\n",
    "        split the image-set into train and val on given split percentage,\n",
    "        creates the data-set folders and copies the image.\n",
    "        If a sample percentage is given, a sub-set is created as sample.\n",
    "        `validate`: True if annotations should be validate, else False\n",
    "        \"\"\"\n",
    "        logger.info('Validation set contains {}% of the images.'.format(int(self.split * 100)))\n",
    "\n",
    "        # validate the image set\n",
    "        skipped_annotations = self.validate() if validate else {}\n",
    "\n",
    "        # split category files into train & val and create the sample split, if set\n",
    "        train_annotation_keys = []\n",
    "        val_annotation_keys = []\n",
    "        sample_train_annotation_keys = []\n",
    "        sample_val_annotation_keys = []\n",
    "\n",
    "        if self.split == 0:\n",
    "            train, val = (list(self.annotations.keys()), [])\n",
    "        elif self.split == 1:\n",
    "            train, val = ([], list(self.annotations.keys()))\n",
    "        else:\n",
    "            train, val = self.split_train_val_data(list(self.annotations.keys()), self.split, self.seed)\n",
    "        train_annotation_keys.extend(train)\n",
    "        val_annotation_keys.extend(val)\n",
    "\n",
    "        # if a sample data set should be created, create the splits\n",
    "        if self.sample:\n",
    "            _, sample_train = self.split_train_val_data(train, self.sample, self.seed)\n",
    "            _, sample_val = self.split_train_val_data(val, self.sample, self.seed)\n",
    "            sample_train_annotation_keys.extend(sample_train)\n",
    "            sample_val_annotation_keys.extend(sample_val)\n",
    "\n",
    "        # if test files exist\n",
    "        test_files = self.input_adapter.list_files(SubsetType.TEST)\n",
    "        if test_files and self.sample:\n",
    "            _, sample_test_files = self.split_train_val_data(test_files, self.sample, self.seed)\n",
    "        else:\n",
    "            sample_test_files = None\n",
    "\n",
    "        # copy the annotations\n",
    "        self.copy(train_annotation_keys, val_annotation_keys, test_files)\n",
    "\n",
    "        if self.sample:\n",
    "            # backup original output path\n",
    "            output_path = self.output_adapter.path\n",
    "            sample_name = \"{}_sample\".format(basename(output_path))\n",
    "            # set output path to sample set\n",
    "            self.output_adapter.path = join(dirname(output_path), sample_name)\n",
    "            logger.info('Start build {} dataset containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                       int(self.sample * 100),\n",
    "                                                                                       self.output_adapter.path))\n",
    "            # create the sample data set folder\n",
    "            create_folder(self.output_adapter.path)\n",
    "            # copy the sample data\n",
    "            self.copy(sample_train_annotation_keys, sample_val_annotation_keys, sample_test_files)\n",
    "\n",
    "            logger.info('Finished build {} dataset containing {}% of images at {}'.format(sample_name,\n",
    "                                                                                          int(self.sample * 100),\n",
    "                                                                                          self.output_adapter.path))\n",
    "            # restore original output path\n",
    "            self.output_adapter.path = output_path\n",
    "\n",
    "    @classmethod\n",
    "    def split_train_val_data(cls, data, val_size=0.2, seed=None):\n",
    "        \"\"\"\n",
    "        Splits the images in train and validation set\n",
    "        `data`: the data to split\n",
    "        `val_size`: the size of the validation set in percentage\n",
    "        `seed`: A random seed to reproduce splits.\n",
    "        return: the split train, validation images\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train, test = train_test_split(data, random_state=seed, test_size=val_size) if len(data) > 1 else (data, [])\n",
    "        return train, test\n",
    "\n",
    "    @classmethod\n",
    "    def assign_orientation(cls, file_path):\n",
    "        \"\"\"\n",
    "        Assign the EXIF metadata orientation to an image.\n",
    "        `file_path`: the path to the image file\n",
    "        \"\"\"\n",
    "\n",
    "        # rotate image by EXIF orientation metadata and remove them\n",
    "        image, exif_data, rotated = assign_exif_orientation(file_path)\n",
    "        if rotated:\n",
    "            write_exif_metadata(image, exif_data, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
