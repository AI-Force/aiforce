{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "from logging.handlers import MemoryHandler\n",
    "from aiforce.core import list_subclasses, parse_known_args_with_help\n",
    "from aiforce import annotation as annotation_package\n",
    "from aiforce.annotation.core import AnnotationAdapter\n",
    "from aiforce import dataset as dataset_package\n",
    "from aiforce.dataset.core import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "logger = logging.getLogger('aiforce.dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generator\n",
    "\n",
    "> Dataset generator Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset for a classification or segmentation task. If an annotation file is present, the annotations are also prepared.\n",
    "The dataset is created based on an imageset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imageset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagesets are collected images to build a data-set from, stored in the `imagesets` folder.\n",
    "The `imagesets` folder contains the following folder structure:\n",
    "- imagesets/*[imageset_type]*/*[imageset_name]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[imageset_name]` folder are the following files / folders\n",
    "- `test/`: test images (benchmark)\n",
    "- `trainval/`: training and validation images for [cross validation](https://pdc-pj.backlog.jp/wiki/RAD_RAD/Neural+Network+-+Training)\n",
    "- `categories.txt`: all categories (classes) the imageset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are stored in the `datasets` base folder.\n",
    "The `datasets` folder contains the following folder structure:\n",
    "- datasets/*[dataset_type]*/*[dataset_name]*\n",
    "where `[dataset_type]` is the same as the corresponding `[imageset_type]` and `[dataset_name]` is the same as the corresponding `[imageset_name]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `[dataset_name]` folder are the following files / folders\n",
    "- `test/`: test set (benchmark)\n",
    "- `train/`: training set\n",
    "- `val/`: validation set\n",
    "- `categories.txt`: all categories (classes) the dataset contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def configure_logging(logging_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configures logging for the system.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging_level)\n",
    "\n",
    "    log_memory_handler = MemoryHandler(1, flushLevel=logging_level)\n",
    "    log_memory_handler.setLevel(logging_level)\n",
    "\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setLevel(logging_level)\n",
    "\n",
    "    logger.addHandler(log_memory_handler)\n",
    "    logger.addHandler(stdout_handler)\n",
    "\n",
    "    logger.setLevel(logging_level)\n",
    "\n",
    "    return log_memory_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data-set from an image-set. Handles currently classification and segmentation image-sets taken from the image-set-type, which is the parent folder, the image-set folder is located in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def generate(dataset: Dataset, log_memory_handler):\n",
    "    \"\"\"\n",
    "    Generate a dataset.\n",
    "    `dataset`: the dataset to build\n",
    "    `log_memory_handler`: the log handler for the build log\n",
    "    \"\"\"\n",
    "    dataset.build_info()\n",
    "\n",
    "    logger.info('Start build {} at {}'.format(type(dataset).__name__, dataset.output_adapter.path))\n",
    "\n",
    "    dataset.create_folders()\n",
    "\n",
    "    # create the build log file\n",
    "    log_file_name = datetime.now().strftime(\"build_%Y.%m.%d-%H.%M.%S.log\")\n",
    "    file_handler = logging.FileHandler(join(dataset.output_adapter.path, log_file_name), encoding=\"utf-8\")\n",
    "    log_memory_handler.setTarget(file_handler)\n",
    "\n",
    "    dataset.build()\n",
    "\n",
    "    logger.info('Finished build {} at {}'.format(type(dataset).__name__, dataset.output_adapter.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the data-set builder from command line, use the following command:\n",
    "`python -m mlcore.dataset [parameters]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are supported:\n",
    "- `[categories]`: The path to the categories file. (e.g.: *imagesets/segmentation/car_damage/categories.txt*)\n",
    "- `--annotation`: The path to the image-set annotation file, the data-set is build from. (e.g.: *imagesets/classification/car_damage/annotations.csv* for classification, *imagesets/segmentation/car_damage/via_region_data.json* for segmentation)\n",
    "- `--split`: The percentage of the data which belongs to validation set, default to *0.2* (=20%)\n",
    "- `--seed`: A random seed to reproduce splits, default to None\n",
    "- `--category-label-key`: The key, the category name can be found in the annotation file, default to *category*.\n",
    "- `--sample`: The percentage of the data which will be copied as a sample set with in a separate folder with \"_sample\" suffix. If not set, no sample data-set will be created.\n",
    "- `--type`: The type of the data-set, if not explicitly set try to infer from categories file path.\n",
    "- `--tfrecord`: Also create .tfrecord files.\n",
    "- `--join-overlapping-regions`: Whether overlapping regions of same category should be joined.\n",
    "- `--annotation-area-thresh`: Keep only annotations with minimum size (width or height) related to image size.\n",
    "- `--output`: The path of the dataset folder, default to *../datasets*.\n",
    "- `--name`: The name of the data-set, if not explicitly set try to infer from categories file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if __name__ == '__main__' and '__file__' in globals():\n",
    "    # for direct shell execution\n",
    "    log_handler = configure_logging()\n",
    "\n",
    "    # read annotation adapters to use\n",
    "    adapters = list_subclasses(annotation_package, AnnotationAdapter)\n",
    "\n",
    "    # read datasets to use\n",
    "    datasets = list_subclasses(dataset_package, Dataset)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\",\n",
    "                        \"--input\",\n",
    "                        help=\"The annotation input adapter.\",\n",
    "                        type=str,\n",
    "                        choices=adapters.keys(),\n",
    "                        required=True)\n",
    "    parser.add_argument(\"-d\",\n",
    "                        \"--dataset\",\n",
    "                        help=\"The dataset to generate.\",\n",
    "                        type=str,\n",
    "                        choices=datasets.keys(),\n",
    "                        required=True)\n",
    "    parser.add_argument(\"-o\",\n",
    "                        \"--output\",\n",
    "                        help=\"The annotation output adapter.\",\n",
    "                        type=str,\n",
    "                        choices=adapters.keys(),\n",
    "                        required=True)\n",
    "\n",
    "    argv = sys.argv\n",
    "    args, argv = parse_known_args_with_help(parser, argv)\n",
    "    input_adapter_class = adapters[args.input]\n",
    "    dataset_class = datasets[args.dataset]\n",
    "    output_adapter_class = adapters[args.output]\n",
    "\n",
    "    # parse the input arguments\n",
    "    input_parser = getattr(input_adapter_class, 'argparse')(prefix='input')\n",
    "    input_args, argv = parse_known_args_with_help(input_parser, argv)\n",
    "\n",
    "    # parse the dataset arguments\n",
    "    dataset_parser = getattr(dataset_class, 'argparse')()\n",
    "    dataset_args, argv = parse_known_args_with_help(dataset_parser, argv)\n",
    "\n",
    "    # parse the output arguments\n",
    "    output_parser = getattr(output_adapter_class, 'argparse')(prefix='output')\n",
    "    output_args, argv = parse_known_args_with_help(output_parser, argv)\n",
    "\n",
    "    input_adapter = input_adapter_class(**vars(input_args))\n",
    "    output_adapter = output_adapter_class(**vars(output_args))\n",
    "    dataset_args.input_adapter = input_adapter\n",
    "    dataset_args.output_adapter = output_adapter\n",
    "    target_dataset = dataset_class(**vars(dataset_args))\n",
    "\n",
    "    logger.info('Build parameters:')\n",
    "    logger.info(' '.join(sys.argv[1:]))\n",
    "\n",
    "    generate(target_dataset, log_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted annotation-core.ipynb.\n",
      "Converted annotation-folder_category_adapter.ipynb.\n",
      "Converted annotation-multi_category_adapter.ipynb.\n",
      "Converted annotation-via_adapter.ipynb.\n",
      "Converted annotation-yolo_adapter.ipynb.\n",
      "Converted annotation_converter.ipynb.\n",
      "Converted annotation_viewer.ipynb.\n",
      "Converted category_tools.ipynb.\n",
      "Converted core.ipynb.\n",
      "Converted dataset-core.ipynb.\n",
      "Converted dataset-image_classification.ipynb.\n",
      "Converted dataset-image_object_detection.ipynb.\n",
      "Converted dataset-image_segmentation.ipynb.\n",
      "Converted dataset-type.ipynb.\n",
      "Converted dataset_generator.ipynb.\n",
      "Converted evaluation-core.ipynb.\n",
      "Converted geometry.ipynb.\n",
      "Converted image-color_palette.ipynb.\n",
      "Converted image-inference.ipynb.\n",
      "Converted image-opencv_tools.ipynb.\n",
      "Converted image-pillow_tools.ipynb.\n",
      "Converted image-tools.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted io-core.ipynb.\n",
      "Converted tensorflow-tflite_converter.ipynb.\n",
      "Converted tensorflow-tflite_metadata.ipynb.\n",
      "Converted tensorflow-tfrecord_builder.ipynb.\n",
      "Converted tools-check_double_images.ipynb.\n",
      "Converted tools-downloader.ipynb.\n",
      "Converted tools-image_size_calculator.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# for generating scripts from notebook directly\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML-Core]",
   "language": "python",
   "name": "conda-env-ML-Core-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
