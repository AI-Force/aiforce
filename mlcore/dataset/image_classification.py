# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/dataset-image_classification.ipynb (unless otherwise specified).

__all__ = ['DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE', 'DEFAULT_SPLIT', 'DATA_SET_FOLDER', 'DATASET_TYPE', 'logger',
           'ImageClassificationDataset', 'build_dataset']

# Cell

import sys
import argparse
import logging
from os.path import join, dirname, basename
from datetime import datetime
from ..annotation.folder_category_adapter import read_annotations as folder_category_read_annotations
from ..annotation.multi_category_adapter import read_annotations as multi_category_read_annotations
from ..annotation.multi_category_adapter import write_annotations as multi_category_write_annotations
from .core import Dataset, configure_logging
from .type import DatasetType

# Cell

DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE = 'annotations.csv'
DEFAULT_SPLIT = 0.2
DATA_SET_FOLDER = 'datasets'
DATASET_TYPE = DatasetType.IMAGE_CLASSIFICATION

# Cell

logger = logging.getLogger(__name__)

# Cell


class ImageClassificationDataset(Dataset):
    """
    Classification dataset.
    `name`: The name of the dataset.
    `base_path`: The dataset base-path.
    `imageset_path`: The imageset source path.
    `categories_path`: The path to the categories.txt file.
    `annotations_path`: The path to the annotations-file.
    """

    def __init__(self, name, base_path, imageset_path, categories_path, annotations_path=None, create_tfrecord=False):
        super().__init__(name, base_path, imageset_path, categories_path, DATASET_TYPE, create_tfrecord)
        self.annotations_path = annotations_path
        # if no annotation file exist, generate annotations based on the folder names
        if not annotations_path:
            self.annotations = folder_category_read_annotations(self.train_val_folder)
        else:
            self.annotations = multi_category_read_annotations(annotations_path, self.train_val_folder)

    def copy(self, train_file_keys, val_file_keys, test_file_names=None):
        """
        Copy the images to the data-set, generate the annotations for train and val images.
        `train_file_keys`: The list of training image keys
        `val_file_keys`: The list of validation image keys
        `test_file_names`: The list of test image file names
        return: A tuple containing train and val annotations
        """

        annotations_train, annotations_val = super().copy(train_file_keys, val_file_keys, test_file_names)

        # write the split train annotations
        if annotations_train:
            annotations_target_path = join(self.train_folder, DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE)
            self.logger.info('Write annotations to {}'.format(annotations_target_path))
            multi_category_write_annotations(annotations_target_path, annotations_train)

        # write the split val annotations
        if annotations_val:
            annotations_target_path = join(self.val_folder, DEFAULT_CLASSIFICATION_ANNOTATIONS_FILE)
            self.logger.info('Write annotations to {}'.format(annotations_target_path))
            multi_category_write_annotations(annotations_target_path, annotations_val)

        return annotations_train, annotations_val

# Cell


def build_dataset(category_file_path, output, annotation_file_path=None, split=DEFAULT_SPLIT, seed=None, sample=0,
                  create_tfrecord=False, dataset_name=None):
    """
    Build the dataset for training, Validation and test
    `category_file_path`: the filename of the categories file
    `output`: the dataset base folder to build the dataset in
    `annotation_file_path`: the file path to the annotation file
    `split`: the size of the validation set as percentage
    `seed`: random seed to reproduce splits
    `sample`: the size of the sample set as percentage
    `create_tfrecord`: Also create .tfrecord files.
    `dataset_name`: the name of the dataset, if not set infer from the category file path
    """
    log_memory_handler = configure_logging()

    path = dirname(category_file_path)

    # try to infer the data-set name if not explicitly set
    if dataset_name is None:
        dataset_name = basename(path)

    logger.info('Build parameters:')
    logger.info(' '.join(sys.argv[1:]))
    logger.info('Build configuration:')
    logger.info('category_file_path: {}'.format(category_file_path))
    logger.info('annotation_file_path: {}'.format(annotation_file_path))
    logger.info('create_tfrecord: {}'.format(create_tfrecord))
    logger.info('split: {}'.format(split))
    logger.info('seed: {}'.format(seed))
    logger.info('sample: {}'.format(sample))
    logger.info('dataset_type: {}'.format(DATASET_TYPE))
    logger.info('output: {}'.format(output))
    logger.info('name: {}'.format(dataset_name))

    logger.info('Start build {} dataset {} at {}'.format(DATASET_TYPE, dataset_name, output))

    dataset = ImageClassificationDataset(dataset_name, output, path, category_file_path, annotation_file_path)

    # create the dataset folders
    logger.info("Start create the dataset folders at {}".format(dataset.base_path))
    dataset.create_folders()
    logger.info("Finished create the dataset folders at {}".format(dataset.base_path))

    # create the build log file
    log_file_name = datetime.now().strftime("build_%Y.%m.%d-%H.%M.%S.log")
    file_handler = logging.FileHandler(join(dataset.folder, log_file_name), encoding="utf-8")
    log_memory_handler.setTarget(file_handler)

    # build the dataset
    dataset.build(split, seed, sample)

    logger.info('Finished build {} dataset {} at {}'.format(DATASET_TYPE, dataset_name, output))

# Cell


if __name__ == '__main__' and '__file__' in globals():
    # for direct shell execution
    parser = argparse.ArgumentParser()
    parser.add_argument("categories",
                        help="The path to the imageset categories file.")
    parser.add_argument("--annotation",
                        help="The path to the imageset annotation file, the dataset is build from.",
                        default=None)
    parser.add_argument("--split",
                        help="Percentage of the data which belongs to validation set.",
                        type=float,
                        default=0.2)
    parser.add_argument("--seed",
                        help="A random seed to reproduce splits.",
                        type=int,
                        default=None)
    parser.add_argument("--sample",
                        help="Percentage of the data which will be copied as a sample set.",
                        type=float,
                        default=0)
    parser.add_argument("--tfrecord",
                        help="Also create .tfrecord files.",
                        action="store_true")
    parser.add_argument("--output",
                        help="The path of the dataset folder.",
                        default=DATA_SET_FOLDER)
    parser.add_argument("--name",
                        help="The name of the dataset, if not explicitly set try to infer from categories file path.",
                        default=None)
    args = parser.parse_args()

    build_dataset(args.categories, args.output, args.annotation, args.split, args.seed, args.sample, args.tfrecord,
                  args.name)
